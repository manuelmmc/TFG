{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9287ae54",
   "metadata": {},
   "source": [
    "ACLARACIÓN: PARA EJECUTAR CORRECTAMENTE EL CÓDIGO DE ESTE NOTEBOOK, ES NECESARIO CAMBIAR LAS RUTAS EN LA QUE SE GUARDAN Y CARGAN LOS DISTINTOS ARCHIVOS QUE SE USAN Y/O CREAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae429d8e",
   "metadata": {},
   "source": [
    "En las siguientes celdas se crean las 24 topologías disitntas para las redes de entrenamineto y las 6 topologías distintas para las redes de test, en ambos casos, siendo creadas mediante el modelo Barabasí-Albert (redes sintéticas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Parámetros para las redes\n",
    "num_nodos = [1000, 3000]  # Número de nodos\n",
    "grados_medios = [2, 4, 8]  # Grado medio de cada nodo\n",
    "num_semillas = 4  # Número de semillas aleatorias\n",
    "\n",
    "# Almacenar las redes generadas\n",
    "redes = []\n",
    "\n",
    "for n in num_nodos:\n",
    "    for k in grados_medios:\n",
    "        for seed in range(num_semillas):\n",
    "            # El número de enlaces que se añaden desde un nuevo nodo a nodos existentes\n",
    "            m = k // 2  # Se divide por 2 porque en el modelo Barabasi-Albert cada enlace añade 2 al grado total\n",
    "            # Establecer la semilla para la generación aleatoria\n",
    "            random_seed = random.randint(0, 10000)  # Genera una semilla aleatoria\n",
    "            # Generar la red Barabasi-Albert con la semilla especificada\n",
    "            G = nx.barabasi_albert_graph(n, m, seed=random_seed)\n",
    "            redes.append(G)\n",
    "            # Opcional: Visualizar la red\n",
    "            # nx.draw(G, with_labels=False, node_size=30)\n",
    "            # plt.show()\n",
    "\n",
    "print(f\"Se han generado {len(redes)} redes Barabasi-Albert con semillas aleatorias.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d227a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parámetros para las redes\n",
    "num_nodos = [2000, 4000]  # Número de nodos\n",
    "grados_medios = [2, 4, 8]  # Grado medio de cada nodo\n",
    "\n",
    "# Almacenar las redes generadas\n",
    "redes_test = []\n",
    "\n",
    "for n in num_nodos:\n",
    "    for k in grados_medios:\n",
    "        # El número de enlaces que se añaden desde un nuevo nodo a nodos existentes\n",
    "        m = k // 2  # Se divide por 2 porque en el modelo Barabasi-Albert cada enlace añade 2 al grado total\n",
    "        # Generar la red Barabasi-Albert\n",
    "        G = nx.barabasi_albert_graph(n, m)\n",
    "        redes_test.append(G)\n",
    "        # Opcional: Visualizar la red\n",
    "        #nx.draw(G, with_labels=False, node_size=30)\n",
    "        #plt.show()\n",
    "\n",
    "print(f\"Se han generado {len(redes_test)} redes Barabasi-Albert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a492fb7",
   "metadata": {},
   "source": [
    "Las siguientes celdas son auxiliares para ayudar a usar siempre las mismas redes de entrenamiento y test, dada la complejidad del entrenamiento de estos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bbef8",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "\n",
    "def guardar_redes(redes, prefijo):\n",
    "    for i, G in enumerate(redes):\n",
    "        nx.write_adjlist(G, f\"{prefijo}_red_{i}.adjlist\")\n",
    "\n",
    "guardar_redes(redes, \"train\")\n",
    "#guardar_redes(redes_test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdf9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def cargar_redes(prefijo, num_redes):\n",
    "    redes = []\n",
    "    for i in range(num_redes):\n",
    "        try:\n",
    "            path = f\"{prefijo}_red_{i}.adjlist\"\n",
    "            G = nx.read_adjlist(path)\n",
    "            redes.append(G)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No se encontró el archivo: {path}\")\n",
    "            break\n",
    "    return redes\n",
    "\n",
    "num_redes_train = 24 \n",
    "num_redes_test = 6  \n",
    "\n",
    "redes = cargar_redes(\"train\", num_redes_train)\n",
    "redes_test = cargar_redes(\"test\", num_redes_test)\n",
    "\n",
    "if not redes or not redes_test:\n",
    "    print(f\"hay algun problema.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a89214",
   "metadata": {},
   "source": [
    "Las siguientes celdas crean las funciones correspondientes para simular la difusión de los modelos SIR e IC, que se usan para crear los ground truth para el entrenamiento de nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4405cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def simulate_SIR_and_generate_labels(G, initial_infecteds, p=0.2, r=0.02, max_steps=100):\n",
    "    # Inicializar todos los nodos a susceptibles\n",
    "    status = {node: 'S' for node in G.nodes()}\n",
    "\n",
    "    # Infectar los nodos iniciales\n",
    "    for node in initial_infecteds:\n",
    "        status[node] = 'I'\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        new_status = dict(status)\n",
    "        for node in G.nodes():\n",
    "            if status[node] == 'I':\n",
    "                # Intentar infectar a los vecinos\n",
    "                for neighbor in G.neighbors(node):\n",
    "                    if status[neighbor] == 'S' and random.random() < p:\n",
    "                        new_status[neighbor] = 'I'\n",
    "                # Recuperar el nodo infectado\n",
    "                if random.random() < r:\n",
    "                    new_status[node] = 'R'\n",
    "        if status == new_status:\n",
    "            break  # Termina si no hay cambios\n",
    "        status = new_status\n",
    "\n",
    "    # Generar etiquetas basadas en el estado final de los nodos\n",
    "    labels = [1 if status[node] == 'I' else 0 for node in G.nodes()]\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296ef985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_IC_and_generate_labels(G, initial_infecteds, p=0.2, max_steps=100):\n",
    "    # Inicializar todos los nodos como susceptibles excepto los infectados iniciales\n",
    "    status = {node: 'S' for node in G.nodes()}\n",
    "    for node in initial_infecteds:\n",
    "        status[node] = 'I'\n",
    "    \n",
    "    newly_infected = set(initial_infecteds)\n",
    "    for step in range(max_steps):\n",
    "        if not newly_infected:\n",
    "            break  # Terminar si no hay nuevos infectados\n",
    "        \n",
    "        new_status = dict(status)\n",
    "        current_newly_infected = set()\n",
    "        for infected_node in newly_infected:\n",
    "            for neighbor in G.neighbors(infected_node):\n",
    "                if status[neighbor] == 'S' and random.random() < p:\n",
    "                    new_status[neighbor] = 'I'\n",
    "                    current_newly_infected.add(neighbor)\n",
    "        \n",
    "        status = new_status\n",
    "        newly_infected = current_newly_infected\n",
    "        \n",
    "        if not newly_infected:\n",
    "            break  # Termina si no hay nuevos infectados en este paso\n",
    "    \n",
    "    # Generar etiquetas basadas en el estado final de los nodos\n",
    "    labels = [1 if status[node] == 'I' else 0 for node in G.nodes()]\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Ahora vamos a generar las etiquetas con el modelo IC para todas las redes\n",
    "# Establecer el tamaño de nodos infectados iniciales a 30\n",
    "#initial_infected_size = 30\n",
    "#results_by_network_IC = []\n",
    "\n",
    "#for G in redes:\n",
    "    # Seleccionar nodos infectados iniciales basado en el tamaño de 30\n",
    "    #initial_infecteds = random.sample(list(G.nodes()), initial_infected_size)\n",
    "    # Simular el modelo SIR\n",
    "    #results = simulate_IC_and_generate_labels(G, initial_infecteds)\n",
    "    # Guardar los resultados\n",
    "    #results_by_network_IC.append(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b722388",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\manue\\\\struc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a808ed",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\manue\\\\struc2vec\\\\src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1dc2e9",
   "metadata": {},
   "source": [
    "En las dos siguientes celdas se ejecuta el metodo de incrustacion de embeddings en las redes de entrenamiento, previamente creando la lista de aristas de las redes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62817fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.struc2vec import Graph\n",
    "import networkx as nx\n",
    "\n",
    "for i, G in enumerate(redes):\n",
    "    nx.write_edgelist(G, f\"red_sintetica_{i}.edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6438b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "ruta_python_env = 'C:\\\\Users\\\\manue\\\\anaconda3\\\\envs\\\\pyg_env\\\\python.exe'\n",
    "ruta_main_struc2vec = 'C:\\\\Users\\\\manue\\\\struc2vec\\\\src\\\\main.py'  # Asumiendo que main.py está en el directorio src\n",
    "\n",
    "for i, _ in enumerate(redes):\n",
    "    comando = f\"{ruta_python_env} {ruta_main_struc2vec} --input red_sintetica_{i}.edgelist --output embeddings_{i}.emb --dimensions 64 --walk-length 70 --num-walks 8 --workers 8 --OPT1 True --OPT2 True --OPT3 True\"\n",
    "    resultado = subprocess.run(comando, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if resultado.returncode != 0:\n",
    "        print(f\"Error en la ejecución para la red {i}: {resultado.stderr}\")\n",
    "    else:\n",
    "        print(f\"Salida para la red {i}: {resultado.stdout}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d6943",
   "metadata": {},
   "source": [
    "Las siguientes celdas se encargan de la creación de las redes reales a través de los ficheros con los datos de las matrices de adyacencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Abrir el archivo y leer las líneas, omitiendo la primera línea de cabecera\n",
    "with open('C:\\\\Users\\\\manue\\\\soc-wiki-Vote.mtx', 'r') as file:\n",
    "    data = file.readlines()[1:]  # Ignora la primera línea con el comentario del MatrixMarket\n",
    "\n",
    "# Preparar listas para filas y columnas\n",
    "rows, cols = [], []\n",
    "\n",
    "# Procesar cada línea excepto la cabecera\n",
    "for line in data:\n",
    "    if line.strip():  # asegurarse de que la línea no esté vacía\n",
    "        parts = line.split()\n",
    "        rows.append(int(parts[0]) - 1)  # Los índices en MTX están basados en 1, los convertimos a base 0\n",
    "        cols.append(int(parts[1]) - 1)\n",
    "\n",
    "# Crear un grafo en NetworkX\n",
    "G = nx.Graph()\n",
    "edges = zip(rows, cols)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Opcional: convertir a formato de PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Crear un grafo vacío en NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\manue\\\\fb-pages-food.edges'\n",
    "\n",
    "# Leer las aristas desde el archivo 'fb-pages-food.edges'\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if not line.startswith('%'):  # Ignorar líneas de comentarios\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                source = int(parts[0])\n",
    "                target = int(parts[1])\n",
    "                G.add_edge(source, target)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3ba0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\manue\\\\web-edu.mtx'\n",
    "\n",
    "# Abrir el archivo y leer las líneas, omitiendo la primera línea de cabecera\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.readlines()[1:]  # Ignora la primera línea con el comentario del MatrixMarket\n",
    "\n",
    "# Preparar listas para filas y columnas\n",
    "rows, cols = [], []\n",
    "\n",
    "# Procesar cada línea excepto la cabecera\n",
    "for line in data:\n",
    "    if line.strip():  # asegurarse de que la línea no esté vacía\n",
    "        parts = line.split()\n",
    "        rows.append(int(parts[0]) - 1)  # Los índices en MTX están basados en 1, los convertimos a base 0\n",
    "        cols.append(int(parts[1]) - 1)\n",
    "\n",
    "# Crear un grafo en NetworkX\n",
    "G = nx.Graph()\n",
    "edges = zip(rows, cols)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Opcional: convertir a formato de PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fe9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\manue\\\\ia-fb-messages.mtx'\n",
    "\n",
    "# Abrir el archivo y leer las líneas, omitiendo la primera línea de cabecera\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.readlines()[1:]  # Ignora la primera línea con el comentario del MatrixMarket\n",
    "\n",
    "# Preparar listas para filas y columnas\n",
    "rows, cols = [], []\n",
    "\n",
    "# Procesar cada línea excepto la cabecera\n",
    "for line in data:\n",
    "    if line.strip():  # asegurarse de que la línea no esté vacía\n",
    "        parts = line.split()\n",
    "        rows.append(int(parts[0]) - 1)  # Los índices en MTX están basados en 1, los convertimos a base 0\n",
    "        cols.append(int(parts[1]) - 1)\n",
    "\n",
    "# Crear un grafo en NetworkX\n",
    "G = nx.Graph()\n",
    "edges = zip(rows, cols)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Opcional: convertir a formato de PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39864455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Nombre del archivo de texto\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\manue\\\\USairport500.txt'\n",
    "\n",
    "# Crear un grafo en NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Abrir el archivo y leer las líneas\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Dividir la línea en sus componentes y convertirlos en enteros\n",
    "        source, target, weight = map(int, line.split())\n",
    "        # Agregar los nodos y las aristas al grafo\n",
    "        G.add_edge(source, target, weight=weight)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec45156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Ruta al archivo que contiene las aristas\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\manue\\\\soc-hamsterster.edges'\n",
    "\n",
    "# Crear un grafo vacío\n",
    "G = nx.Graph()\n",
    "\n",
    "# Leer el archivo de aristas y agregarlas al grafo\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if not line.startswith('%'):  # Ignorar líneas de comentarios\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                source = int(parts[0])\n",
    "                target = int(parts[1])\n",
    "                G.add_edge(source, target)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae1c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Crear un grafo vacío en NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\manue\\\\soc-sign-bitcoinalpha.csv'\n",
    "\n",
    "# Leer el archivo CSV y agregar las aristas al grafo\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(',')\n",
    "        source = int(parts[0])\n",
    "        target = int(parts[1])\n",
    "        G.add_edge(source, target)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9c11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from scipy.io import mmread\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Leer el archivo MTX\n",
    "matrix = mmread('C:\\\\Users\\\\manue\\\\econ-mahindas.mtx')\n",
    "\n",
    "# Convertir la matriz dispersa en un grafo NetworkX\n",
    "G = nx.Graph(matrix)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a4238",
   "metadata": {},
   "source": [
    "Las siguientes celdas, al igual que otras anteriores ya comentadas, ejecutan el método struc2vec en las redes de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.struc2vec import Graph\n",
    "import networkx as nx\n",
    "\n",
    "for i, G in enumerate(redes_test):\n",
    "    nx.write_edgelist(G, f\"red_sintetica_{i}_test.edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "ruta_python_env = 'C:\\\\Users\\\\manue\\\\anaconda3\\\\envs\\\\pyg_env\\\\python.exe'\n",
    "ruta_main_struc2vec = 'C:\\\\Users\\\\manue\\\\struc2vec\\\\src\\\\main.py'  # Asumiendo que main.py está en el directorio src\n",
    "\n",
    "# Ejecuta struc2vec solo para las primeras 6 redes\n",
    "for i in range(6):\n",
    "    comando = f\"{ruta_python_env} {ruta_main_struc2vec} --input red_sintetica_{i}_test.edgelist --output embeddings_{i}_test.emb --dimensions 64 --walk-length 70 --num-walks 8 --workers 8 --OPT1 True --OPT2 True --OPT3 True\"\n",
    "    resultado = subprocess.run(comando, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if resultado.returncode != 0:\n",
    "        print(f\"Error en la ejecución para la red {i}: {resultado.stderr}\")\n",
    "    else:\n",
    "        print(f\"Salida para la red {i}: {resultado.stdout}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a013f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.struc2vec import Graph\n",
    "import networkx as nx\n",
    "import subprocess\n",
    "\n",
    "ruta_python_env = 'C:\\\\Users\\\\manue\\\\anaconda3\\\\envs\\\\pyg_env\\\\python.exe'\n",
    "ruta_main_struc2vec = 'C:\\\\Users\\\\manue\\\\struc2vec\\\\src\\\\main.py'\n",
    "\n",
    "def save_edgelist(graph, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for edge in graph.edge_index.T.tolist():\n",
    "            file.write(f\"{edge[0]} {edge[1]}\\n\")\n",
    "\n",
    "indice_inicio = len(redes_test) - 8\n",
    "\n",
    "for i in range(indice_inicio, len(redes_test)):\n",
    "    save_edgelist(redes_test[i], f\"red_sintetica_{i}_test.edgelist\")\n",
    "\n",
    "\n",
    "for i in range(indice_inicio, len(redes_test)):    \n",
    "    comando = f\"{ruta_python_env} {ruta_main_struc2vec} --input red_sintetica_{i}_test.edgelist --output embeddings_{i}_test.emb --dimensions 128 --walk-length 80 --num-walks 10 --workers 8 --OPT1 True --OPT2 True --OPT3 True\"\n",
    "    resultado = subprocess.run(comando, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if resultado.returncode != 0:\n",
    "        print(f\"Error en la ejecución para la red {i}: {resultado.stderr}\")\n",
    "    else:\n",
    "        print(f\"Salida para la red {i}: {resultado.stdout}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad1a8e",
   "metadata": {},
   "source": [
    "Las siguientes celdas simulan la difusión con las funciones anteriormente comentadas en las 24 topologías de red de entrenamiento para los distintos tamaños escogidos (10,30 y 50) para el conjunto de semillas iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e098ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "data_list_SIR = []  # Para almacenar objetos Data de todas las redes\n",
    "seed_sizes = [10, 30, 50]  # Tamaños de semillas iniciales\n",
    "\n",
    "for i in range(len(redes)):  # Asumimos que 'redes' contiene todas las redes\n",
    "    # Cargar la red sintética\n",
    "    G = nx.read_edgelist(f'red_sintetica_{i}.edgelist', nodetype=int)\n",
    "    \n",
    "    # Cargar los embeddings generados por struc2vec o algún otro método\n",
    "    embeddings = np.loadtxt(f'embeddings_{i}.emb', skiprows=1)\n",
    "    node_features = embeddings[:, 1:]  # Las características (embeddings) de los nodos\n",
    "    \n",
    "    # Crear un tensor de características\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Crear listas de aristas\n",
    "    edge_index = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    for seed_size in seed_sizes:\n",
    "        # Generar etiquetas usando la función de simulación SIR o IC modificada\n",
    "        initial_infecteds = random.sample(list(G.nodes()), seed_size)  # Escoger nodos al azar como infectados iniciales\n",
    "        labels = simulate_SIR_and_generate_labels(G, initial_infecteds)\n",
    "        \n",
    "        y = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Crear el objeto Data con x, edge_index y y\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data_list_SIR.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d23d6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "data_list_SI = []  # Para almacenar objetos Data de todas las redes\n",
    "seed_sizes = [10, 30, 50]  # Tamaños de semillas inSIiales\n",
    "\n",
    "for i in range(len(redes)):  # Asumimos que 'redes' contiene todas tus redes\n",
    "    # Cargar la red sintética\n",
    "    G = nx.read_edgelist(f'red_sintetica_{i}.edgelist', nodetype=int)\n",
    "    \n",
    "    # Cargar los embeddings generados por struc2vec o algún otro método\n",
    "    embeddings = np.loadtxt(f'embeddings_{i}.emb', skiprows=1)\n",
    "    node_features = embeddings[:, 1:]  # Las características (embeddings) de los nodos\n",
    "    \n",
    "    # Crear un tensor de características\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Crear listas de aristas\n",
    "    edge_index = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    for seed_size in seed_sizes:\n",
    "        # Generar etiquetas usando la función de simulación SI modificada\n",
    "        initial_infecteds = random.sample(list(G.nodes()), seed_size)  # Escoger nodos al azar como infectados iniciales\n",
    "        labels = simulate_SIR_and_generate_labels(G, initial_infecteds, r=0)\n",
    "        \n",
    "        y = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Crear el objeto Data con x, edge_index y y\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data_list_SI.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f36876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "data_list_IC = []  # Para almacenar objetos Data de todas las redes\n",
    "seed_sizes = [10, 30, 50]  # Tamaños de semillas iniciales\n",
    "\n",
    "for i in range(len(redes)):  # Asumimos que 'redes' contiene todas tus redes\n",
    "    # Cargar la red sintética\n",
    "    G = nx.read_edgelist(f'red_sintetica_{i}.edgelist', nodetype=int)\n",
    "    \n",
    "    # Cargar los embeddings generados por struc2vec o algún otro método\n",
    "    embeddings = np.loadtxt(f'embeddings_{i}.emb', skiprows=1)\n",
    "    node_features = embeddings[:, 1:]  # Las características (embeddings) de los nodos\n",
    "    \n",
    "    # Crear un tensor de características\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Crear listas de aristas\n",
    "    edge_index = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    for seed_size in seed_sizes:\n",
    "        # Generar etiquetas usando la función de simulación IC o IC modificada\n",
    "        initial_infecteds = random.sample(list(G.nodes()), seed_size)  # Escoger nodos al azar como infectados iniciales\n",
    "        labels = simulate_IC_and_generate_labels(G, initial_infecteds)\n",
    "        \n",
    "        y = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Crear el objeto Data con x, edge_index y y\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data_list_IC.append(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b881f",
   "metadata": {},
   "source": [
    "Las siguientes celdas se encargan de separar las redes de entrenamiento en 2 conjuntos, 70% para train y 30% para validación. Además se ejecutan comandos auxiliares para poder realizar lo anterior y para poder realizar el entrenamiento en 5 scripts distintos para ser utilizados en 5 GPUs distintas. En este código veríamos como se ejecuta el primer script, siendo el resto de scripts iguales, cambiando el valor de new_index a 1, 2, 3 o 4 respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb047b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sizes = [10, 30, 50]\n",
    "num_redes = 24\n",
    "\n",
    "# Asegúrate de que existe un directorio para los embeddings modificados\n",
    "os.makedirs('modified_embeddings', exist_ok=True)\n",
    "\n",
    "# Nuevo índice para el archivo de salida\n",
    "new_index = 0\n",
    "\n",
    "for i in range(num_redes):\n",
    "    original_file = f'embeddings_{i}.emb'\n",
    "    # Leer el contenido original si es necesario\n",
    "    original_content = open(original_file, 'r').read()\n",
    "    for size in sizes:\n",
    "        new_file = f'modified_embeddings/embeddings_{new_index}.emb'\n",
    "        with open(new_file, 'w') as file:\n",
    "            file.write(original_content)\n",
    "        new_index += 1  # Incrementa el índice para cada nuevo archivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "# Supongamos que tienes 72 redes y sus índices de archivos van del 0 al 71\n",
    "indices = np.arange(72)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Divide los índices en entrenamiento y validación\n",
    "train_indices = indices[:int(0.7 * len(indices))]  # 70% para entrenamiento\n",
    "val_indices = indices[int(0.7 * len(indices)):]  # 30% para validación\n",
    "\n",
    "# Suponiendo que `data_list_SIR`, `data_list_SI`, y `data_list_IC` son listas de tus 72 redes\n",
    "train_data_SIR = [data_list_SIR[i] for i in train_indices]\n",
    "val_data_SIR = [data_list_SIR[i] for i in val_indices]\n",
    "\n",
    "train_data_SI = [data_list_SI[i] for i in train_indices]\n",
    "val_data_SI = [data_list_SI[i] for i in val_indices]\n",
    "\n",
    "train_data_IC = [data_list_IC[i] for i in train_indices]\n",
    "val_data_IC = [data_list_IC[i] for i in val_indices]\n",
    "\n",
    "\n",
    "\n",
    "# Crea directorios para guardar los datos divididos y los chunks\n",
    "os.makedirs('train_data_chunks', exist_ok=True)\n",
    "os.makedirs('val_data', exist_ok=True)\n",
    "\n",
    "# Guarda cada chunk en un archivo separado\n",
    "chunk_size = 10  # Número de redes por chunk\n",
    "train_data_chunks_SIR = [train_data_SIR[i:i + chunk_size] for i in range(0, len(train_data_SIR), chunk_size)]\n",
    "train_data_chunks_SI = [train_data_SI[i:i + chunk_size] for i in range(0, len(train_data_SI), chunk_size)]\n",
    "train_data_chunks_IC = [train_data_IC[i:i + chunk_size] for i in range(0, len(train_data_IC), chunk_size)]\n",
    "\n",
    "index_to_chunk_map = {index: idx // chunk_size for idx, index in enumerate(train_indices)}\n",
    "\n",
    "for i in range(len(train_data_chunks_SIR)):\n",
    "    chunk_dir = f'train_data_chunks/chunk_{i}'\n",
    "    os.makedirs(chunk_dir, exist_ok=True)\n",
    "    \n",
    "    with open(f'{chunk_dir}/train_data_chunk_SIR.pkl', 'wb') as f:\n",
    "        pickle.dump(train_data_chunks_SIR[i], f)\n",
    "        \n",
    "    with open(f'{chunk_dir}/train_data_chunk_SI.pkl', 'wb') as f:\n",
    "        pickle.dump(train_data_chunks_SI[i], f)  \n",
    "    \n",
    "    with open(f'{chunk_dir}/train_data_chunk_IC.pkl', 'wb') as f:\n",
    "        pickle.dump(train_data_chunks_IC[i], f)            \n",
    "\n",
    "# Copia los archivos de embeddings correspondientes a los directorios de entrenamiento y validación\n",
    "for idx in train_indices:\n",
    "    chunk_idx = index_to_chunk_map[idx]\n",
    "    shutil.copy(f'modified_embeddings/embeddings_{idx}.emb', f'train_data_chunks/chunk_{chunk_idx}/')\n",
    "    \n",
    "for idx in val_indices:\n",
    "    shutil.copy(f'modified_embeddings/embeddings_{idx}.emb', 'val_data/')\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d979f6",
   "metadata": {},
   "source": [
    "La siguiente celda se queda comentada pues es una celda auxiliar usada una vez el entrenamiento ya había sido realizado en las distinas GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a924533",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lista de archivos de embeddings en val_data\n",
    "embedding_files = ['embeddings_0.emb', 'embeddings_10.emb', 'embeddings_11.emb', 'embeddings_14.emb', 'embeddings_15.emb', \n",
    "                   'embeddings_16.emb', 'embeddings_17.emb', 'embeddings_18.emb', 'embeddings_2.emb', 'embeddings_20.emb', \n",
    "                   'embeddings_21.emb', 'embeddings_22.emb', 'embeddings_23.emb', 'embeddings_24.emb', 'embeddings_27.emb', \n",
    "                   'embeddings_28.emb', 'embeddings_29.emb', 'embeddings_3.emb', 'embeddings_30.emb', 'embeddings_32.emb', \n",
    "                   'embeddings_33.emb', 'embeddings_35.emb', 'embeddings_36.emb', 'embeddings_37.emb', 'embeddings_38.emb', \n",
    "                   'embeddings_39.emb', 'embeddings_4.emb', 'embeddings_40.emb', 'embeddings_41.emb', 'embeddings_42.emb', \n",
    "                   'embeddings_43.emb', 'embeddings_44.emb', 'embeddings_45.emb', 'embeddings_46.emb', 'embeddings_47.emb', \n",
    "                   'embeddings_48.emb', 'embeddings_49.emb', 'embeddings_5.emb', 'embeddings_50.emb', 'embeddings_52.emb', \n",
    "                   'embeddings_53.emb', 'embeddings_54.emb', 'embeddings_57.emb', 'embeddings_58.emb', 'embeddings_59.emb', \n",
    "                   'embeddings_60.emb', 'embeddings_61.emb', 'embeddings_62.emb', 'embeddings_63.emb', 'embeddings_64.emb', \n",
    "                   'embeddings_65.emb', 'embeddings_66.emb', 'embeddings_67.emb', 'embeddings_68.emb', 'embeddings_69.emb', \n",
    "                   'embeddings_70.emb', 'embeddings_71.emb', 'embeddings_8.emb', 'embeddings_9.emb']\n",
    "\n",
    "# Extraer los índices de los archivos de embeddings\n",
    "val_indices = [int(f.split('_')[1].split('.')[0]) for f in embedding_files]\n",
    "\n",
    "# Crear las listas de validación para SIR, SI e IC\n",
    "val_data_SIR = [data_list_SIR[i] for i in val_indices]\n",
    "val_data_SI = [data_list_SI[i] for i in val_indices]\n",
    "val_data_IC = [data_list_IC[i] for i in val_indices]\n",
    "\n",
    "print(\"val_data_SIR:\", val_data_SIR)\n",
    "print(\"val_data_SI:\", val_data_SI)\n",
    "print(\"val_data_IC:\", val_data_IC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72957a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo de datos de entrenamiento\n",
    "chunk_index = 0  # Cambia este índice para los otros archivos\n",
    "chunk_dir = f'train_data_chunks/chunk_{chunk_index}'\n",
    "\n",
    "train_data_file_SIR = f'{chunk_dir}/train_data_chunk_SIR.pkl'\n",
    "train_data_file_SI = f'{chunk_dir}/train_data_chunk_SI.pkl'\n",
    "train_data_file_IC = f'{chunk_dir}/train_data_chunk_IC.pkl'\n",
    "\n",
    "# Cargar los datos de entrenamiento desde los archivos\n",
    "with open(train_data_file_SIR, 'rb') as f:\n",
    "    train_data_SIR = pickle.load(f)    \n",
    "\n",
    "with open(train_data_file_SI, 'rb') as f:\n",
    "    train_data_SI = pickle.load(f) \n",
    "\n",
    "with open(train_data_file_IC, 'rb') as f:\n",
    "    train_data_IC = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bda6cb",
   "metadata": {},
   "source": [
    "La siguiente celda crea la arquitectura de capas para el primer modelo de GNN a usar, GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca12666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "from torch.nn import Dropout  # Corregido\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.bn1 = BatchNorm(hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = BatchNorm(hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn3 = BatchNorm(hidden_dim)\n",
    "        self.conv4 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = Dropout(dropout_rate)  # Usar el parámetro de dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63691ff8",
   "metadata": {},
   "source": [
    "Las siguientes celdas ejecutan el entrenamiento de GCN en las redes de los distintos modelos de difusión (SIR, SI e IC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GCN_SIR = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_SIR):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GNN(input_dim=data.num_features, hidden_dim=hidden_dim, output_dim=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_SIR_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GCN_SIR.append(model_path)\n",
    "        print(f\"Modelo SIR {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24988d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GCN_SI = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_SI):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GNN(input_dim=data.num_features, hidden_dim=hidden_dim, output_dim=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_SI_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GCN_SI.append(model_path)\n",
    "        print(f\"Modelo SI {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f017ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GCN_IC = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_IC):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GNN(input_dim=data.num_features, hidden_dim=hidden_dim, output_dim=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_IC_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GCN_IC.append(model_path)\n",
    "        print(f\"Modelo IC {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc88d9",
   "metadata": {},
   "source": [
    "Las siguientes funciones calculan el total de nodos infectados a través de los modelos de difusión en la red que se le pasa como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d8a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def simular_difusion_SIR(G, semillas, p=0.05, r=0.02, max_steps=100):\n",
    "    status = {node: 'S' for node in G.nodes()}\n",
    "    for node in semillas:\n",
    "        status[node] = 'I'\n",
    "    \n",
    "    #print(f\"Estado inicial (semillas infectadas): {semillas}\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        new_status = dict(status)\n",
    "        infectados = 0\n",
    "        recuperados = 0\n",
    "        for node in G.nodes():\n",
    "            if status[node] == 'I':\n",
    "                for neighbor in G.neighbors(node):\n",
    "                    if status[neighbor] == 'S' and random.random() < p:\n",
    "                        new_status[neighbor] = 'I'\n",
    "                        infectados += 1\n",
    "                if random.random() < r:\n",
    "                    new_status[node] = 'R'\n",
    "                    recuperados += 1\n",
    "        status = new_status\n",
    "        #print(f\"Paso {step + 1}: Infectados nuevos = {infectados}, Recuperados = {recuperados}\")\n",
    "        \n",
    "        if infectados == 0 and recuperados == 0:\n",
    "            #print(\"No hay más cambios en el estado de los nodos.\")\n",
    "            break\n",
    "    \n",
    "    total_infectados = sum(1 for state in status.values() if state in ['I', 'R'])\n",
    "    #print(f\"Total infectados al final: {total_infectados}\")\n",
    "    return total_infectados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "858e8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simular_difusion_IC(G, semillas, p=0.05, max_steps=100):\n",
    "    status = {node: 'S' for node in G.nodes()}\n",
    "    for node in semillas:\n",
    "        if node not in G:\n",
    "            print(f\"El nodo {node} no está en el grafo.\")\n",
    "        status[node] = 'I'\n",
    "    \n",
    "    newly_infected = set(semillas)\n",
    "    #print(f\"Estado inicial (semillas infectadas): {semillas}\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if not newly_infected:\n",
    "     #       print(\"No hay nuevos infectados.\")\n",
    "            break\n",
    "        \n",
    "        new_status = dict(status)\n",
    "        current_newly_infected = set()\n",
    "        for infected_node in newly_infected:\n",
    "            if infected_node not in G:\n",
    "                continue  # Salta a la siguiente iteración si el nodo no existe\n",
    "            for neighbor in G.neighbors(infected_node):\n",
    "                if status[neighbor] == 'S' and np.random.random() < p:\n",
    "                    new_status[neighbor] = 'I'\n",
    "                    current_newly_infected.add(neighbor)\n",
    "        \n",
    "        status = new_status\n",
    "        newly_infected = current_newly_infected\n",
    "        #print(f\"Paso {step + 1}: Nuevos infectados = {len(newly_infected)}\")\n",
    "        \n",
    "    total_infectados = sum(1 for state in status.values() if state == 'I')\n",
    "    #print(f\"Total infectados al final: {total_infectados}\")\n",
    "    return total_infectados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85203634",
   "metadata": {},
   "source": [
    "La siguiente es una celda auxiliar usada una vez se ha realizado el entrenamiento de los distintos modelos. Por tanto, se deja comentada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1c31a",
   "metadata": {},
   "source": [
    "# Inicialización de listas para cada combinación de modelo y tipo de difusión\n",
    "modelos_paths_GCN_SIR = []\n",
    "modelos_paths_GCN_SI = []\n",
    "modelos_paths_GCN_IC = []\n",
    "\n",
    "modelos_paths_GAT_SIR = []\n",
    "modelos_paths_GAT_SI = []\n",
    "modelos_paths_GAT_IC = []\n",
    "\n",
    "modelos_paths_GraphSAGE_SIR = []\n",
    "modelos_paths_GraphSAGE_SI = []\n",
    "modelos_paths_GraphSAGE_IC = []\n",
    "\n",
    "# Lista con los nombres de los archivos de modelos\n",
    "model_names = [\n",
    "    \"Nuevo_model_GAT_IC_0_0_lr0.001_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_1_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_2_lr0.01_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_3_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_4_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_5_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_6_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_7_lr0.001_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_8_lr0.001_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_0_9_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_0_lr0.01_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_1_lr0.001_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_2_lr0.001_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_3_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_4_lr0.001_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_5_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_6_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_7_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_8_lr0.001_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_1_9_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_0_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_1_lr0.001_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_2_lr0.005_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_3_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_4_lr0.01_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_5_lr0.001_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_6_lr0.001_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_7_lr0.01_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_8_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_2_9_lr0.01_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_0_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_1_lr0.005_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_2_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_3_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_4_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_5_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_6_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_7_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_8_lr0.001_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_3_9_lr0.001_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_0_lr0.001_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_1_lr0.001_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_2_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_3_lr0.01_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_4_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_5_lr0.001_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_6_lr0.001_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_7_lr0.01_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_8_lr0.001_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_IC_4_9_lr0.001_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_0_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_1_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_2_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_3_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_4_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_5_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_6_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_7_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_8_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_0_9_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_0_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_1_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_2_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_3_lr0.005_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_4_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_5_lr0.01_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_6_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_7_lr0.01_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_8_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SI_1_9_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_0_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_1_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_2_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_3_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_4_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_5_lr0.001_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_6_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_7_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_8_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SI_2_9_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_0_lr0.01_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_1_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_2_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_3_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_4_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_5_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_6_lr0.01_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_7_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_8_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_3_9_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_0_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_1_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_2_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_3_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_4_lr0.01_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_5_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_6_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_7_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_8_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SI_4_9_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_0_lr0.001_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_1_lr0.001_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_2_lr0.001_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_3_lr0.005_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_4_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_5_lr0.001_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_6_lr0.005_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_7_lr0.005_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_8_lr0.005_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_0_9_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_0_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_1_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_2_lr0.001_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_3_lr0.001_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_4_lr0.001_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_5_lr0.01_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_6_lr0.001_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_7_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_8_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_1_9_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_0_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_1_lr0.01_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_2_lr0.01_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_3_lr0.005_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_4_lr0.001_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_5_lr0.001_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_6_lr0.005_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_7_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_8_lr0.005_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_2_9_lr0.001_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_0_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_1_lr0.005_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_2_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_3_lr0.001_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_4_lr0.001_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_5_lr0.005_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_6_lr0.001_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_7_lr0.001_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_8_lr0.001_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_3_9_lr0.01_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_0_lr0.001_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_1_lr0.001_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_2_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_3_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_4_lr0.005_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_5_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_6_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_7_lr0.01_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_8_lr0.001_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GAT_SIR_4_9_lr0.001_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_0_lr0.005_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_1_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_2_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_3_lr0.001_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_4_lr0.01_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_5_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_6_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_7_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_8_lr0.005_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_0_9_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_0_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_1_lr0.005_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_2_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_3_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_4_lr0.01_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_5_lr0.005_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_6_lr0.005_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_7_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_8_lr0.001_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_1_9_lr0.005_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_0_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_1_lr0.005_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_2_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_3_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_4_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_5_lr0.01_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_6_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_7_lr0.005_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_8_lr0.01_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_2_9_lr0.001_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_0_lr0.001_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_1_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_2_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_3_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_4_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_5_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_6_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_7_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_8_lr0.001_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_3_9_lr0.001_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_0_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_1_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_2_lr0.001_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_3_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_4_lr0.001_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_5_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_6_lr0.001_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_7_lr0.01_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_8_lr0.001_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_IC_4_9_lr0.01_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_0_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_1_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_2_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_3_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_4_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_5_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_6_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_7_lr0.005_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_8_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_0_9_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_0_lr0.01_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_1_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_2_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_3_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_4_lr0.005_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_5_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_6_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_7_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_8_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_1_9_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_0_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_1_lr0.01_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_2_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_3_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_4_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_5_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_6_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_7_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_8_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_2_9_lr0.01_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_0_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_1_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_2_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_3_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_4_lr0.01_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_5_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_6_lr0.01_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_7_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_8_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_3_9_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_0_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_1_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_2_lr0.001_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_3_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_4_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_5_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_6_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_7_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_8_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SI_4_9_lr0.01_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_0_lr0.005_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_1_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_2_lr0.001_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_3_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_4_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_5_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_6_lr0.01_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_7_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_8_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_0_9_lr0.005_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_0_lr0.01_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_1_lr0.001_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_2_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_3_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_4_lr0.001_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_5_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_6_lr0.005_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_7_lr0.001_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_8_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_1_9_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_0_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_1_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_2_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_3_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_4_lr0.005_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_5_lr0.005_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_6_lr0.001_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_7_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_8_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_2_9_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_0_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_1_lr0.01_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_2_lr0.01_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_3_lr0.005_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_4_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_5_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_6_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_7_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_8_lr0.005_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_3_9_lr0.005_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_0_lr0.01_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_1_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_2_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_3_lr0.005_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_4_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_5_lr0.001_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_6_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_7_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_8_lr0.005_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_GraphSAGE_SIR_4_9_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_0_0_lr0.01_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_0_1_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_0_2_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_0_3_lr0.001_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_0_4_lr0.005_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_0_5_lr0.005_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_0_6_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_0_7_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_0_8_lr0.005_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_0_9_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_1_0_lr0.01_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_1_1_lr0.005_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_1_2_lr0.001_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_1_3_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_1_4_lr0.01_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_1_5_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_1_6_lr0.001_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_1_7_lr0.01_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_1_8_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_1_9_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_2_0_lr0.01_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_2_1_lr0.005_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_2_2_lr0.005_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_2_3_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_2_4_lr0.001_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_2_5_lr0.01_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_2_6_lr0.001_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_2_7_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_2_8_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_2_9_lr0.005_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_3_0_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_3_1_lr0.01_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_3_2_lr0.01_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_3_3_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_3_4_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_3_5_lr0.005_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_3_6_lr0.001_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_3_7_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_3_8_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_3_9_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_4_0_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_4_1_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_4_2_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_4_3_lr0.005_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_IC_4_4_lr0.005_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_4_5_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_IC_4_6_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_4_7_lr0.001_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_4_8_lr0.005_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_IC_4_9_lr0.001_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_0_0_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_0_1_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_0_2_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_0_3_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_0_4_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_0_5_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SI_0_6_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_0_7_lr0.005_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_0_8_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_0_9_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_1_0_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_1_1_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_1_2_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_1_3_lr0.005_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_SI_1_4_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_1_5_lr0.005_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_1_6_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_1_7_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_1_8_lr0.005_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_1_9_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SI_2_0_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_2_1_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_2_2_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_2_3_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_2_4_lr0.001_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_2_5_lr0.01_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SI_2_6_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_2_7_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_2_8_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_2_9_lr0.005_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_3_0_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_3_1_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SI_3_2_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_3_3_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_3_4_lr0.01_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_SI_3_5_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_3_6_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_3_7_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_3_8_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_3_9_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_4_0_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_4_1_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_4_2_lr0.001_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_4_3_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_4_4_lr0.005_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SI_4_5_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_4_6_lr0.01_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_4_7_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_4_8_lr0.01_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SI_4_9_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_0_0_lr0.01_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_0_1_lr0.01_wd0.001_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_0_2_lr0.01_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_0_3_lr0.01_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_0_4_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_0_5_lr0.005_wd0.01_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_0_6_lr0.01_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_0_7_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_0_8_lr0.01_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_0_9_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_1_0_lr0.001_wd0.001_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_1_1_lr0.01_wd0.001_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_1_2_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_1_3_lr0.005_wd0.01_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_1_4_lr0.001_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_1_5_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_1_6_lr0.001_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_1_7_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_1_8_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_1_9_lr0.005_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_2_0_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_2_1_lr0.01_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_2_2_lr0.001_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_2_3_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_2_4_lr0.01_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_2_5_lr0.01_wd0.1_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_2_6_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_2_7_lr0.005_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_2_8_lr0.005_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_2_9_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_3_0_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_3_1_lr0.01_wd0.01_hd64_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_3_2_lr0.005_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_3_3_lr0.01_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_3_4_lr0.01_wd0.1_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_3_5_lr0.001_wd0.01_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_3_6_lr0.005_wd0.001_hd128_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_3_7_lr0.005_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_3_8_lr0.001_wd0.001_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_3_9_lr0.01_wd0.1_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_4_0_lr0.005_wd0.01_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_4_1_lr0.01_wd0.001_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_4_2_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_4_3_lr0.001_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_4_4_lr0.005_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_4_5_lr0.005_wd0.1_hd128_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_4_6_lr0.005_wd0.1_hd128_dp0.4.pt\",\n",
    "    \"Nuevo_model_SIR_4_7_lr0.005_wd0.01_hd64_dp0.2.pt\",\n",
    "    \"Nuevo_model_SIR_4_8_lr0.01_wd0.1_hd64_dp0.6.pt\",\n",
    "    \"Nuevo_model_SIR_4_9_lr0.01_wd0.001_hd128_dp0.2.pt\"\n",
    "  ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Función para asignar nombres de archivos a las listas correspondientes\n",
    "def asignar_modelo_a_lista(filename):\n",
    "    if \"GAT\" in filename:\n",
    "        if \"SIR\" in filename:\n",
    "            modelos_paths_GAT_SIR.append(filename)\n",
    "        elif \"SI\" in filename:\n",
    "            modelos_paths_GAT_SI.append(filename)\n",
    "        elif \"IC\" in filename:\n",
    "            modelos_paths_GAT_IC.append(filename)\n",
    "    elif \"GraphSAGE\" in filename:\n",
    "        if \"SIR\" in filename:\n",
    "            modelos_paths_GraphSAGE_SIR.append(filename)\n",
    "        elif \"SI\" in filename:\n",
    "            modelos_paths_GraphSAGE_SI.append(filename)\n",
    "        elif \"IC\" in filename:\n",
    "            modelos_paths_GraphSAGE_IC.append(filename)\n",
    "    elif \"SIR\" in filename and \"GAT\" not in filename and \"GraphSAGE\" not in filename:\n",
    "        modelos_paths_GCN_SIR.append(filename)\n",
    "    elif \"SI\" in filename and \"GAT\" not in filename and \"GraphSAGE\" not in filename:\n",
    "        modelos_paths_GCN_SI.append(filename)\n",
    "    elif \"IC\" in filename and \"GAT\" not in filename and \"GraphSAGE\" not in filename:\n",
    "        modelos_paths_GCN_IC.append(filename)\n",
    "\n",
    "# Asignar cada nombre de archivo a la lista correspondiente\n",
    "for model_name in model_names:\n",
    "    asignar_modelo_a_lista(model_name)\n",
    "\n",
    "# Mostrar los resultados en una tabla\n",
    "modelos_dict = {\n",
    "    \"GCN_SIR\": modelos_paths_GCN_SIR,\n",
    "    \"GCN_SI\": modelos_paths_GCN_SI,\n",
    "    \"GCN_IC\": modelos_paths_GCN_IC,\n",
    "    \"GAT_SIR\": modelos_paths_GAT_SIR,\n",
    "    \"GAT_SI\": modelos_paths_GAT_SI,\n",
    "    \"GAT_IC\": modelos_paths_GAT_IC,\n",
    "    \"GraphSAGE_SIR\": modelos_paths_GraphSAGE_SIR,\n",
    "    \"GraphSAGE_SI\": modelos_paths_GraphSAGE_SI,\n",
    "    \"GraphSAGE_IC\": modelos_paths_GraphSAGE_IC\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "modelos_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in modelos_dict.items()]))\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(modelos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053103b6",
   "metadata": {},
   "source": [
    "Las siguientes funciones sirven para cargar los modelos resultantes del entrenamiento, para los modelos GCN entrenados con los distintos modelos de difusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d988a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "def cargar_modelo_GCN_SIR(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_SIR_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo interpretar los hiperparámetros del nombre del archivo para SIR\")\n",
    "\n",
    "    model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GCN_IC(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_IC_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo interpretar los hiperparámetros del nombre del archivo para IC\")\n",
    "\n",
    "    model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GCN_SI(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_SI_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo interpretar los hiperparámetros del nombre del archivo para SI\")\n",
    "\n",
    "    model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5038e",
   "metadata": {},
   "source": [
    "Las siguientes celdas realizan la selección de la mejor red de entrenamiento usando para ello las redes de validación según la ecuación mostrada en la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_SIR = [cargar_modelo_GCN_SIR(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GCN_SIR]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_SIR = np.zeros((len(modelos_entrenados_SIR), len(val_data_SIR)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_SIR):\n",
    "    for j, data_test in enumerate(val_data_SIR):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_SIR[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_SIR = resultados_influencia_SIR.mean(axis=1)\n",
    "mejor_red_idx_GCN_SIR = np.argmax(promedio_influencia_por_red_SIR)\n",
    "print(f\"La mejor red de entrenamiento SIR es la número {mejor_red_idx_GCN_SIR + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_SIR[mejor_red_idx_GCN_SIR]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_SI = [cargar_modelo_GCN_SI(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GCN_SI]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_SI = np.zeros((len(modelos_entrenados_SI), len(val_data_SI)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_SI):\n",
    "    for j, data_test in enumerate(val_data_SI):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas, r=0)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_SI[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_SI = resultados_influencia_SI.mean(axis=1)\n",
    "mejor_red_idx_GCN_SI = np.argmax(promedio_influencia_por_red_SI)\n",
    "print(f\"La mejor red de entrenamiento SI es la número {mejor_red_idx_GCN_SI + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_SI[mejor_red_idx_GCN_SI]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83531fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_IC = [cargar_modelo_GCN_IC(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GCN_IC]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_IC = np.zeros((len(modelos_entrenados_IC), len(val_data_IC)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_IC):\n",
    "    for j, data_test in enumerate(val_data_IC):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_IC(G_test, semillas)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_IC[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_IC = resultados_influencia_IC.mean(axis=1)\n",
    "mejor_red_idx_GCN_IC = np.argmax(promedio_influencia_por_red_IC)\n",
    "print(f\"La mejor red de entrenamiento IC es la número {mejor_red_idx_GCN_IC + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_IC[mejor_red_idx_GCN_IC]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224cb750",
   "metadata": {},
   "source": [
    "La siguiente es una celda auxiliar usada una vez realizada la selección de la mejor red de entrenamiento en los distintos modelos de difusión. Por ello se deja comentada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26535167",
   "metadata": {},
   "source": [
    "mejor_red_idx_GCN_SIR = 8\n",
    "mejor_red_idx_GCN_SI = 30\n",
    "mejor_red_idx_GCN_IC = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9cae0",
   "metadata": {},
   "source": [
    "Ahora se vuelve a hacer de nuevo lo mismo que se ha hecho con GCN para los modelos GAT y GrapSAGE (entrenamiento, funciones para cargar los modelos entrenados y selección de la mejor red de entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4029b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8, dropout_rate=0.4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        self.gat1 = GATv2Conv(input_dim, hidden_dim, heads=heads, concat=True)\n",
    "        self.gat2 = GATv2Conv(hidden_dim * heads, output_dim, heads=1, concat=False)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GAT_SIR = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_SIR):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GAT(input_dim=data.num_features, hidden_dim=hidden_dim, output_dim=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_GAT_SIR_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GAT_SIR.append(model_path)\n",
    "        print(f\"Modelo SIR {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GAT_SI = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_SI):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GAT(input_dim=data.num_features, hidden_dim=hidden_dim, output_dim=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_GAT_SI_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GAT_SI.append(model_path)\n",
    "        print(f\"Modelo SI {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92afca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GAT_IC = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_IC):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GAT(input_dim=data.num_features, hidden_dim=hidden_dim, output_dim=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_GAT_IC_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GAT_IC.append(model_path)\n",
    "        print(f\"Modelo IC {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71911048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "def cargar_modelo_GAT_SIR(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_GAT_SIR_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo extraer los hiperparámetros del nombre del archivo para SIR\")\n",
    "    \n",
    "    model = GAT(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, heads=8, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GAT_SI(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_GAT_SI_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo extraer los hiperparámetros del nombre del archivo para SI\")\n",
    "    \n",
    "    model = GAT(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, heads=8, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GAT_IC(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_GAT_IC_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo interpretar los hiperparámetros del nombre del archivo para IC\")\n",
    "    \n",
    "    model = GAT(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, heads=8, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f30fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_SIR = [cargar_modelo_GAT_SIR(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GAT_SIR]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_SIR = np.zeros((len(modelos_entrenados_SIR), len(val_data_SIR)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_SIR):\n",
    "    for j, data_test in enumerate(val_data_SIR):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_SIR[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_SIR = resultados_influencia_SIR.mean(axis=1)\n",
    "mejor_red_idx_GAT_SIR = np.argmax(promedio_influencia_por_red_SIR)\n",
    "print(f\"La mejor red de entrenamiento SIR es la número {mejor_red_idx_GAT_SIR + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_SIR[mejor_red_idx_GAT_SIR]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_SI = [cargar_modelo_GAT_SI(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GAT_SI]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_SI = np.zeros((len(modelos_entrenados_SI), len(val_data_SI)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_SI):\n",
    "    for j, data_test in enumerate(val_data_SI):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas, r=0)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_SI[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_SI = resultados_influencia_SI.mean(axis=1)\n",
    "mejor_red_idx_GAT_SI = np.argmax(promedio_influencia_por_red_SI)\n",
    "print(f\"La mejor red de entrenamiento SI es la número {mejor_red_idx_GAT_SI + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_SI[mejor_red_idx_GAT_SI]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc448a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_IC = [cargar_modelo_GAT_IC(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GAT_IC]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_IC = np.zeros((len(modelos_entrenados_IC), len(val_data_IC)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_IC):\n",
    "    for j, data_test in enumerate(val_data_IC):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_IC(G_test, semillas)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_IC[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_IC = resultados_influencia_IC.mean(axis=1)\n",
    "mejor_red_idx_GAT_IC = np.argmax(promedio_influencia_por_red_IC)\n",
    "print(f\"La mejor red de entrenamiento IC es la número {mejor_red_idx_GAT_IC + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_IC[mejor_red_idx_GAT_IC]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb143e7",
   "metadata": {},
   "source": [
    "mejor_red_idx_GAT_SIR = 20\n",
    "mejor_red_idx_GAT_SI = 48\n",
    "mejor_red_idx_GAT_IC = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e8b4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout_rate=0.5):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        # Capa de entrada\n",
    "        self.convs.append(SAGEConv(input_dim, hidden_dim))\n",
    "        \n",
    "        # Capas intermedias\n",
    "        for i in range(1, num_layers-1):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Capa de salida\n",
    "        self.convs.append(SAGEConv(hidden_dim, output_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Configurable dropout probability\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded4b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GraphSAGE_SIR = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_SIR):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GraphSAGE(in_features=data.num_features, hidden_dim=hidden_dim, out_features=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data.x, data.edge_index)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_GraphSAGE_SIR_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GraphSAGE_SIR.append(model_path)\n",
    "        print(f\"Modelo SIR {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GraphSAGE_SI = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_SI):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GraphSAGE(in_features=data.num_features, hidden_dim=hidden_dim, out_features=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data.x, data.edge_index)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_GraphSAGE_SI_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GraphSAGE_SI.append(model_path)\n",
    "        print(f\"Modelo SI {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3605a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, BatchNorm\n",
    "from torch.nn import Dropout, CrossEntropyLoss\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "modelos_paths_GraphSAGE_IC = []\n",
    "\n",
    "# Define los rangos de hiperparámetros\n",
    "lrs = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.1, 0.01, 0.001]\n",
    "hidden_dims = [64, 128]\n",
    "dropout_rates = [0.2, 0.4, 0.6]  # Agregar más valores según se considere necesario\n",
    "patience = 50  # Número de épocas a esperar antes de detener el entrenamiento si no hay mejora\n",
    "\n",
    "\n",
    "# Bucle sobre cada red en el conjunto de entrenamiento\n",
    "for idx, data in enumerate(train_data_IC):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Variables para guardar el mejor modelo y error de cada configuración\n",
    "    best_global_val_loss = float('inf')\n",
    "    best_global_params = {}\n",
    "\n",
    "    # Prueba todas las combinaciones de hiperparámetros\n",
    "    for lr, weight_decay, hidden_dim, dropout_rate in itertools.product(lrs, weight_decays, hidden_dims, dropout_rates):\n",
    "        print(f\"Testing combination: lr={lr}, wd={weight_decay}, hd={hidden_dim}, dp={dropout_rate}\")\n",
    "        model = GraphSAGE(in_features=data.num_features, hidden_dim=hidden_dim, out_features=2, dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Crear máscaras de entrenamiento y validación\n",
    "        train_mask = torch.rand(num_nodes) < 0.8\n",
    "        val_mask = ~train_mask\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        \n",
    "        patience_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(1000):  # Considera usar un número menor de épocas si el entrenamiento es muy largo\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data.x, data.edge_index)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Guardar el mejor modelo para esta combinación de parámetros\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} with best val_loss {best_val_loss}\")\n",
    "                break\n",
    "\n",
    "        # Comparar contra el mejor error de validación global\n",
    "        if best_val_loss < best_global_val_loss:\n",
    "            best_global_val_loss = best_val_loss\n",
    "            best_global_params = {'lr': lr, 'weight_decay': weight_decay, 'hidden_dim': hidden_dim, 'dropout': dropout_rate, 'idx': idx}\n",
    "            best_global_model = best_model\n",
    "\n",
    "    if best_global_model is not None:\n",
    "        model_path = f\"Nuevo_model_GraphSAGE_IC_{chunk_index}_{idx}_lr{best_global_params['lr']}_wd{best_global_params['weight_decay']}_hd{best_global_params['hidden_dim']}_dp{best_global_params['dropout']}.pt\"\n",
    "        torch.save(best_global_model.state_dict(), model_path)\n",
    "        modelos_paths_GraphSAGE_IC.append(model_path)\n",
    "        print(f\"Modelo IC {idx+1} guardado en {model_path} con lr={best_global_params['lr']}, weight_decay={best_global_params['weight_decay']}, hidden_dim={best_global_params['hidden_dim']}, dropout={best_global_params['dropout']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4df9f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "def cargar_modelo_GraphSAGE_SIR(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_GraphSAGE_SIR_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo extraer los hiperparámetros del nombre del archivo para SIR\")\n",
    "    \n",
    "    model = GraphSAGE(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GraphSAGE_SI(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_GraphSAGE_SI_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo extraer los hiperparámetros del nombre del archivo para SI\")\n",
    "    \n",
    "    model = GraphSAGE(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GraphSAGE_IC(path, input_dim, output_dim, device):\n",
    "    match = re.search(r'Nuevo_model_GraphSAGE_IC_\\d+_\\d+_lr([\\d\\.]+)_wd([\\d\\.]+)_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        lr = float(match.group(1))\n",
    "        weight_decay = float(match.group(2))\n",
    "        hidden_dim = int(match.group(3))\n",
    "        dropout_rate = float(match.group(4))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo interpretar los hiperparámetros del nombre del archivo para IC\")\n",
    "    \n",
    "    model = GraphSAGE(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_SIR = [cargar_modelo_GraphSAGE_SIR(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GraphSAGE_SIR]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_SIR = np.zeros((len(modelos_entrenados_SIR), len(val_data_SIR)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_SIR):\n",
    "    for j, data_test in enumerate(val_data_SIR):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        # Asegúrate de que los datos estén en el mismo dispositivo que el modelo\n",
    "        data_test = Data(x=x_test, edge_index=edge_index).to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test.x, data_test.edge_index)  # Asegúrate de usar 'data_test'\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_SIR[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_SIR = resultados_influencia_SIR.mean(axis=1)\n",
    "mejor_red_idx_GraphSAGE_SIR = np.argmax(promedio_influencia_por_red_SIR)\n",
    "print(f\"La mejor red de entrenamiento SIR es la número {mejor_red_idx_GraphSAGE_SIR + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_SIR[mejor_red_idx_GraphSAGE_SIR]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_SI = [cargar_modelo_GraphSAGE_SI(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GraphSAGE_SI]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_SI = np.zeros((len(modelos_entrenados_SI), len(val_data_SI)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_SI):\n",
    "    for j, data_test in enumerate(val_data_SI):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index).to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test.x, data_test.edge_index)  # Usar data_test aquí\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas, r=0)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_SI[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_SI = resultados_influencia_SI.mean(axis=1)\n",
    "mejor_red_idx_GraphSAGE_SI = np.argmax(promedio_influencia_por_red_SI)\n",
    "print(f\"La mejor red de entrenamiento SI es la número {mejor_red_idx_GraphSAGE_SI + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_SI[mejor_red_idx_GraphSAGE_SI]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d68b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\"Verifica si el objeto es un string que representa un dígito.\"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelos_entrenados_IC = [cargar_modelo_GraphSAGE_IC(path, input_dim=64, output_dim=2, device=device) for path in modelos_paths_GraphSAGE_IC]\n",
    "\n",
    "# Asumir que 'val_data' es una lista de objetos Data utilizados para la validación\n",
    "resultados_influencia_IC = np.zeros((len(modelos_entrenados_IC), len(val_data_IC)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_IC):\n",
    "    for j, data_test in enumerate(val_data_IC):\n",
    "        idx = val_indices[j]\n",
    "        G_test = to_networkx(data_test, to_undirected=True)\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'val_data/embeddings_{idx}.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index).to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test.x, data_test.edge_index)  # Usar data_test aquí\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_IC(G_test, semillas)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_IC[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_IC = resultados_influencia_IC.mean(axis=1)\n",
    "mejor_red_idx_GraphSAGE_IC = np.argmax(promedio_influencia_por_red_IC)\n",
    "print(f\"La mejor red de entrenamiento IC es la número {mejor_red_idx_GraphSAGE_IC + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_IC[mejor_red_idx_GraphSAGE_IC]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27160898",
   "metadata": {},
   "source": [
    "\n",
    "mejor_red_idx_GraphSAGE_SIR = 17\n",
    "mejor_red_idx_GraphSAGE_SI = 34\n",
    "mejor_red_idx_GraphSAGE_IC = 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4adb2f",
   "metadata": {},
   "source": [
    "La siguiente celda crea las funciones para cargar los modelos de las mejores redes de entrenamiento según el índice que ocupa en la lista de redes de entrenamiento de cada modelo de difusión distinto. Cada funcion sirve para cargar los modelos según la variante de GNN utilizada (GCN, GAT o GraphSAGE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4426027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "def cargar_modelo_GCN_generic(paths, idx, input_dim, output_dim, device):\n",
    "    path = paths[idx]\n",
    "    match = re.search(r'Nuevo_model_(\\w+)_\\d+_\\d+_lr[\\d\\.]+_wd[\\d\\.]+_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        hidden_dim = int(match.group(2))\n",
    "        dropout_rate = float(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo interpretar los hiperparámetros del nombre del archivo\")\n",
    "    \n",
    "    model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GAT_generic(paths, idx, input_dim, output_dim, device):\n",
    "    path = paths[idx]\n",
    "    match = re.search(r'Nuevo_model_GAT_(\\w+)_\\d+_\\d+_lr[\\d\\.]+_wd[\\d\\.]+_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        hidden_dim = int(match.group(2))\n",
    "        dropout_rate = float(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo interpretar los hiperparámetros del nombre del archivo\")\n",
    "    \n",
    "    model = GAT(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, heads=8, dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GraphSAGE_generic(paths, idx, input_dim, output_dim, device):\n",
    "    path = paths[idx]\n",
    "    match = re.search(r'Nuevo_model_GraphSAGE_(\\w+)_\\d+_\\d+_lr[\\d\\.]+_wd[\\d\\.]+_hd(\\d+)_dp([\\d\\.]+).pt', path)\n",
    "    if match:\n",
    "        hidden_dim = int(match.group(2))\n",
    "        dropout_rate = float(match.group(3))\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo interpretar los hiperparámetros del nombre del archivo\")\n",
    "    \n",
    "    model = GraphSAGE(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=3, dropout_rate=dropout_rate)  # Assuming fixed 3 layers\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49ac39",
   "metadata": {},
   "source": [
    "La siguiente celda crea las gráficas que muestran la influencia promedio de cada red de test según los distintos tamaños del conjunto de semillas iniciales ejecutados (10,15,....,50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e4e887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def seleccionar_semillas(modelo, data, size, model_type):\n",
    "    modelo.eval()\n",
    "    with torch.no_grad():\n",
    "        if model_type == 'GCN' or model_type == 'GAT':\n",
    "            out = modelo(data)  # Para GNN que espera un objeto Data\n",
    "        else:\n",
    "            out = modelo(data.x, data.edge_index)  # Para GAT y GraphSAGE que esperan x y edge_index por separado\n",
    "        influencia_scores = torch.softmax(out, dim=1)[:, 1]\n",
    "        _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "        semillas = indices_semillas.tolist()\n",
    "    return semillas\n",
    "\n",
    "# Modelos de GNN\n",
    "modelos_gnn_sir = {\n",
    "    'GCN': (cargar_modelo_GCN_generic(modelos_paths_GCN_SIR, mejor_red_idx_GCN_SIR, 64, 2, device), 'GCN'),\n",
    "    'GAT': (cargar_modelo_GAT_generic(modelos_paths_GAT_SIR, mejor_red_idx_GAT_SIR, 64, 2, device), 'GAT'),\n",
    "    'GraphSAGE': (cargar_modelo_GraphSAGE_generic(modelos_paths_GraphSAGE_SIR, mejor_red_idx_GraphSAGE_SIR, 64, 2, device), 'GraphSAGE')\n",
    "}\n",
    "\n",
    "modelos_gnn_ic = {\n",
    "    'GCN': (cargar_modelo_GCN_generic(modelos_paths_GCN_IC, mejor_red_idx_GCN_IC, 64, 2, device), 'GCN'),\n",
    "    'GAT': (cargar_modelo_GAT_generic(modelos_paths_GAT_IC, mejor_red_idx_GAT_IC, 64, 2, device), 'GAT'),\n",
    "    'GraphSAGE': (cargar_modelo_GraphSAGE_generic(modelos_paths_GraphSAGE_IC, mejor_red_idx_GraphSAGE_IC, 64, 2, device), 'GraphSAGE')\n",
    "}\n",
    "\n",
    "modelos_gnn_si = {\n",
    "    'GCN': (cargar_modelo_GCN_generic(modelos_paths_GCN_SI, mejor_red_idx_GCN_SI, 64, 2, device), 'GCN'),\n",
    "    'GAT': (cargar_modelo_GAT_generic(modelos_paths_GAT_SI, mejor_red_idx_GAT_SI, 64, 2, device), 'GAT'),\n",
    "    'GraphSAGE': (cargar_modelo_GraphSAGE_generic(modelos_paths_GraphSAGE_SI, mejor_red_idx_GraphSAGE_SI, 64, 2, device), 'GraphSAGE')\n",
    "}\n",
    "\n",
    "\n",
    "# Función para seleccionar semillas usando modelos clásicos\n",
    "def seleccionar_semillas_clasico(G, size, model_type):\n",
    "    if model_type == 'degree':\n",
    "        # Seleccionar nodos con mayor grado\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        semillas = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:size]\n",
    "    elif model_type == 'betweenness':\n",
    "        # Seleccionar nodos con mayor centralidad de intermediación\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        semillas = sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)[:size]\n",
    "    return semillas\n",
    "\n",
    "# Añadir modelos clásicos a la lista de modelos\n",
    "modelos_clasicos = {\n",
    "    'Degree': 'degree',\n",
    "    'Betweenness': 'betweenness'\n",
    "}\n",
    "\n",
    "sizes = range(10, 51, 5)\n",
    "\n",
    "# Preparar DataFrame para guardar los resultados\n",
    "columnas = [f'Size_{size}' for size in sizes]\n",
    "indice_modelos_SIR = ['GCN_SIR', 'GAT_SIR', 'GraphSAGE_SIR', 'Degree', 'Betweenness']\n",
    "df_resultados_SIR = pd.DataFrame(columns=columnas, index=indice_modelos_SIR)\n",
    "df_resultados_SIR_sinteticas = pd.DataFrame(columns=columnas, index=indice_modelos_SIR)\n",
    "\n",
    "indice_modelos_SI = ['GCN_SI', 'GAT_SI', 'GraphSAGE_SI', 'Degree', 'Betweenness']\n",
    "df_resultados_SI = pd.DataFrame(columns=columnas, index=indice_modelos_SI)\n",
    "df_resultados_SI_sinteticas = pd.DataFrame(columns=columnas, index=indice_modelos_SI)\n",
    "\n",
    "indice_modelos_IC = ['GCN_IC', 'GAT_IC', 'GraphSAGE_IC', 'Degree', 'Betweenness']\n",
    "df_resultados_IC = pd.DataFrame(columns=columnas, index=indice_modelos_IC)\n",
    "df_resultados_IC_sinteticas = pd.DataFrame(columns=columnas, index=indice_modelos_IC)\n",
    "\n",
    "\n",
    "\n",
    "def plot_influencia(resultados, sizes, title, index, diffusion_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for key, values in resultados.items():\n",
    "        plt.plot(sizes, values, label=key, marker='o')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Número de Semillas Iniciales', fontsize=14)\n",
    "    plt.ylabel('Influencia Promedio Normalizada', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./resultados_TFG/Grafica_Influencia_{diffusion_type}_todos_vs_inicial_Red_{index}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_influencia_01(resultados, sizes, title, index, diffusion_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for key, values in resultados.items():\n",
    "        plt.plot(sizes, values, label=key, marker='o')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Número de Semillas Iniciales', fontsize=16)\n",
    "    plt.ylabel('Influencia Promedio Normalizada', fontsize=16)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./resultados_TFG/Grafica_Influencia_{diffusion_type}_todos_vs_inicial_Red_{index}_01.png')\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "nombres_redes = [\n",
    "    \"Wiki\",\n",
    "    \"Fb-Food\",\n",
    "    \"Web-edu\",\n",
    "    \"Fb-Messages\",\n",
    "    \"USAir 500\",\n",
    "    \"Hamster\",\n",
    "    \"Bitcoin\",\n",
    "    \"Mahindas\"\n",
    "]\n",
    "\n",
    "    \n",
    "    \n",
    "# #Inicializar DataFrame para acumular resultados\n",
    "df_acumulado_SIR_sinteticas = pd.DataFrame(columns=[f'Size_{size}' for size in sizes])    \n",
    "\n",
    "# Proceso para cada red Barabási-Albert indexada de 0 a 5\n",
    "for index in range(6):  # Asumiendo que index recorre las redes Barabási-Albert\n",
    "    resultados = {modelo_name: [] for modelo_name in {**modelos_gnn_sir, **modelos_clasicos}}\n",
    "    G_test = redes_test[index]  # Suponemos que esto ya es un grafo de networkx\n",
    "\n",
    "    # Convertir G_test de networkx a Data de PyTorch Geometric si necesario\n",
    "    if not isinstance(G_test, Data):\n",
    "        G_test = from_networkx(G_test)\n",
    "    \n",
    "    G_test = to_networkx(G_test, to_undirected=True)  # Así nos aseguramos de que es manipulable como un grafo de networkx\n",
    "    G_test = nx.relabel_nodes(G_test, {n: int(n) for n in G_test.nodes()})\n",
    "\n",
    "    num_nodes = len(G_test.nodes)\n",
    "    edge_index = torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    for modelo_name in resultados.keys():\n",
    "        for size in sizes:\n",
    "            if modelo_name in modelos_clasicos:\n",
    "                semillas = seleccionar_semillas_clasico(G_test, size, modelos_clasicos[modelo_name])\n",
    "            else:\n",
    "                modelo, model_type = modelos_gnn_sir[modelo_name]\n",
    "                data_test = Data(x=torch.tensor(np.random.rand(len(G_test.nodes()), 64), dtype=torch.float).to(device),\n",
    "                                 edge_index=torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous().to(device))\n",
    "                semillas = seleccionar_semillas(modelo, data_test, size, model_type)\n",
    "\n",
    "            infectados_total = 0\n",
    "            num_runs = 10\n",
    "            for _ in range(num_runs):\n",
    "                infectados = simular_difusion_SIR(G_test, semillas)\n",
    "                infectados_total += infectados\n",
    "            influencia_promedio = infectados_total / num_runs / len(G_test.nodes())\n",
    "            resultados[modelo_name].append(influencia_promedio)\n",
    "\n",
    "    # Convertir los resultados a DataFrame y sumar al acumulado\n",
    "    df_resultados_temp = pd.DataFrame(resultados, index=[f'Size_{size}' for size in sizes]).T\n",
    "    if df_acumulado_SIR_sinteticas.empty:\n",
    "        df_acumulado_SIR_sinteticas = df_resultados_temp\n",
    "    else:\n",
    "        df_acumulado_SIR_sinteticas += df_resultados_temp\n",
    "\n",
    "    \n",
    "    \n",
    "    # Generar gráfica para cada red\n",
    "    plot_influencia_01(resultados, list(sizes), f'Influencia Promedio en SIR vs. Tamaño de Semillas Iniciales para red sintética', index, 'SIR')\n",
    "\n",
    "    \n",
    "# Calcular el promedio de influencia para cada modelo y tamaño de semilla\n",
    "df_resultados_promedio_SIR_sinteticas = df_acumulado_SIR_sinteticas / 8\n",
    "\n",
    "# Guardar los resultados promedio\n",
    "df_resultados_promedio_SIR_sinteticas.to_csv('./resultados_TFG/resultados_influencia_normalizada_promedio_SIR_sinteticas.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Inicializar DataFrame para acumular resultados\n",
    "df_acumulado_SIR = pd.DataFrame(columns=[f'Size_{size}' for size in sizes])\n",
    "    \n",
    "for index in range(6, 14):\n",
    "    resultados = {modelo_name: [] for modelo_name in {**modelos_gnn_sir, **modelos_clasicos}}\n",
    "    G_test = redes_test[index]\n",
    "    G_test = to_networkx(G_test, to_undirected=True)\n",
    "    G_test = nx.relabel_nodes(G_test, {n: int(n) for n in G_test.nodes()})\n",
    "\n",
    "    for modelo_name in resultados.keys():\n",
    "        for size in sizes:\n",
    "            if modelo_name in modelos_clasicos:\n",
    "                semillas = seleccionar_semillas_clasico(G_test, size, modelos_clasicos[modelo_name])\n",
    "            else:\n",
    "                modelo, model_type = modelos_gnn_sir[modelo_name]\n",
    "                data_test = Data(x=torch.tensor(np.random.rand(len(G_test.nodes()), 64), dtype=torch.float).to(device),\n",
    "                                 edge_index=torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous().to(device))\n",
    "                semillas = seleccionar_semillas(modelo, data_test, size, model_type)\n",
    "\n",
    "            infectados_total = 0\n",
    "            num_runs = 10\n",
    "            for _ in range(num_runs):\n",
    "                infectados = simular_difusion_SIR(G_test, semillas)\n",
    "                infectados_total += infectados\n",
    "            influencia_promedio = infectados_total / num_runs / len(G_test.nodes())\n",
    "            resultados[modelo_name].append(influencia_promedio)\n",
    "\n",
    "    # Convertir los resultados a DataFrame y sumar al acumulado\n",
    "    df_resultados_temp = pd.DataFrame(resultados, index=[f'Size_{size}' for size in sizes]).T\n",
    "    if df_acumulado_SIR.empty:\n",
    "        df_acumulado_SIR = df_resultados_temp\n",
    "    else:\n",
    "        df_acumulado_SIR += df_resultados_temp\n",
    "\n",
    "    nombre_red = nombres_redes[index - 6]\n",
    "    \n",
    "    # Generar gráfica para cada red\n",
    "    plot_influencia_01(resultados, list(sizes), f'Influencia Promedio en SIR vs. Tamaño de Semillas Iniciales para {nombre_red}', index, 'SIR')\n",
    "\n",
    "    \n",
    "# Calcular el promedio de influencia para cada modelo y tamaño de semilla\n",
    "df_resultados_promedio_SIR = df_acumulado_SIR / 8\n",
    "\n",
    "# Guardar los resultados promedio\n",
    "df_resultados_promedio_SIR.to_csv('./resultados_TFG/resultados_influencia_normalizada_promedio_SIR.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Inicializar DataFrame para acumular resultados\n",
    "df_acumulado_SI_sinteticas = pd.DataFrame(columns=[f'Size_{size}' for size in sizes])\n",
    "\n",
    "# Proceso para cada red Barabási-Albert indexada de 0 a 5\n",
    "for index in range(6):  # Asumiendo que index recorre las redes Barabási-Albert\n",
    "    resultados = {modelo_name: [] for modelo_name in {**modelos_gnn_si, **modelos_clasicos}}\n",
    "    G_test = redes_test[index]  # Suponemos que esto ya es un grafo de networkx\n",
    "\n",
    "    # Convertir G_test de networkx a Data de PyTorch Geometric si necesario\n",
    "    if not isinstance(G_test, Data):\n",
    "        G_test = from_networkx(G_test)\n",
    "    \n",
    "    G_test = to_networkx(G_test, to_undirected=True)  # Así nos aseguramos de que es manipulable como un grafo de networkx\n",
    "    G_test = nx.relabel_nodes(G_test, {n: int(n) for n in G_test.nodes()})\n",
    "\n",
    "    num_nodes = len(G_test.nodes)\n",
    "    edge_index = torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    for modelo_name in resultados.keys():\n",
    "        for size in sizes:\n",
    "            if modelo_name in modelos_clasicos:\n",
    "                semillas = seleccionar_semillas_clasico(G_test, size, modelos_clasicos[modelo_name])\n",
    "            else:\n",
    "                modelo, model_type = modelos_gnn_si[modelo_name]\n",
    "                data_test = Data(x=torch.tensor(np.random.rand(len(G_test.nodes()), 64), dtype=torch.float).to(device),\n",
    "                                 edge_index=torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous().to(device))\n",
    "                semillas = seleccionar_semillas(modelo, data_test, size, model_type)\n",
    "\n",
    "            infectados_total = 0\n",
    "            num_runs = 10\n",
    "            for _ in range(num_runs):\n",
    "                infectados = simular_difusion_SIR(G_test, semillas, r=0)\n",
    "                infectados_total += infectados\n",
    "            influencia_promedio = infectados_total / num_runs / len(G_test.nodes())\n",
    "            resultados[modelo_name].append(influencia_promedio)\n",
    "\n",
    "    # Convertir los resultados a DataFrame y sumar al acumulado\n",
    "    df_resultados_temp = pd.DataFrame(resultados, index=[f'Size_{size}' for size in sizes]).T\n",
    "    if df_acumulado_SI_sinteticas.empty:\n",
    "        df_acumulado_SI_sinteticas = df_resultados_temp\n",
    "    else:\n",
    "        df_acumulado_SI_sinteticas += df_resultados_temp\n",
    "\n",
    "    # Generar gráfica para cada red\n",
    "    plot_influencia(resultados, list(sizes), f'Influencia Promedio en SI vs. Tamaño de Semillas Iniciales para Red {index}', index, 'SI')\n",
    "    plot_influencia_01(resultados, list(sizes), f'Influencia Promedio en SI vs. Tamaño de Semillas Iniciales para Red {index}', index, 'SI')\n",
    "\n",
    "    \n",
    "# Calcular el promedio de influencia para cada modelo y tamaño de semilla\n",
    "df_resultados_promedio_SI_sinteticas = df_acumulado_SI_sinteticas / 6\n",
    "\n",
    "# Guardar los resultados promedio\n",
    "df_resultados_promedio_SI_sinteticas.to_csv('./resultados_TFG/resultados_influencia_normalizada_promedio_SI_sinteticas.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Inicializar DataFrame para acumular resultados\n",
    "df_acumulado_SI = pd.DataFrame(columns=[f'Size_{size}' for size in sizes])\n",
    "\n",
    "# Bucle sobre las redes\n",
    "for index in range(6, 14):\n",
    "    resultados = {modelo_name: [] for modelo_name in {**modelos_gnn_si, **modelos_clasicos}}\n",
    "    G_test = redes_test[index]\n",
    "    G_test = to_networkx(G_test, to_undirected=True)\n",
    "    G_test = nx.relabel_nodes(G_test, {n: int(n) for n in G_test.nodes()})\n",
    "\n",
    "    for modelo_name in resultados.keys():\n",
    "        for size in sizes:\n",
    "            if modelo_name in modelos_clasicos:\n",
    "                semillas = seleccionar_semillas_clasico(G_test, size, modelos_clasicos[modelo_name])\n",
    "            else:\n",
    "                modelo, model_type = modelos_gnn_si[modelo_name]\n",
    "                data_test = Data(x=torch.tensor(np.random.rand(len(G_test.nodes()), 64), dtype=torch.float).to(device),\n",
    "                                 edge_index=torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous().to(device))\n",
    "                semillas = seleccionar_semillas(modelo, data_test, size, model_type)\n",
    "\n",
    "            infectados_total = 0\n",
    "            num_runs = 10\n",
    "            for _ in range(num_runs):\n",
    "                infectados = simular_difusion_SIR(G_test, semillas, r=0)\n",
    "                infectados_total += infectados\n",
    "            influencia_promedio = infectados_total / num_runs / len(G_test.nodes())\n",
    "            resultados[modelo_name].append(influencia_promedio)\n",
    "\n",
    "    # Convertir los resultados a DataFrame y sumar al acumulado\n",
    "    df_resultados_temp = pd.DataFrame(resultados, index=[f'Size_{size}' for size in sizes]).T\n",
    "    if df_acumulado_SI.empty:\n",
    "        df_acumulado_SI = df_resultados_temp\n",
    "    else:\n",
    "        df_acumulado_SI += df_resultados_temp\n",
    "\n",
    "    # Generar gráfica para cada red\n",
    "    plot_influencia(resultados, list(sizes), f'Influencia Promedio en SI vs. Tamaño de Semillas Iniciales para Red {index}', index, 'SI')\n",
    "    plot_influencia_01(resultados, list(sizes), f'Influencia Promedio en SI vs. Tamaño de Semillas Iniciales para Red {index}', index, 'SI')\n",
    "\n",
    "    \n",
    "# Calcular el promedio de influencia para cada modelo y tamaño de semilla\n",
    "df_resultados_promedio_SI = df_acumulado_SI / 8\n",
    "\n",
    "# Guardar los resultados promedio\n",
    "df_resultados_promedio_SI.to_csv('./resultados_TFG/resultados_influencia_normalizada_promedio_SI.csv')\n",
    "   \n",
    "    \n",
    "#Inicializar DataFrame para acumular resultados\n",
    "df_acumulado_IC_sinteticas = pd.DataFrame(columns=[f'Size_{size}' for size in sizes])    \n",
    "\n",
    "# Proceso para cada red Barabási-Albert indexada de 0 a 5\n",
    "for index in range(6):  # Asumiendo que index recorre las redes Barabási-Albert\n",
    "    resultados = {modelo_name: [] for modelo_name in {**modelos_gnn_ic, **modelos_clasicos}}\n",
    "    G_test = redes_test[index]  # Suponemos que esto ya es un grafo de networkx\n",
    "\n",
    "    # Convertir G_test de networkx a Data de PyTorch Geometric si necesario\n",
    "    if not isinstance(G_test, Data):\n",
    "        G_test = from_networkx(G_test)\n",
    "    \n",
    "    G_test = to_networkx(G_test, to_undirected=True)  # Así nos aseguramos de que es manipulable como un grafo de networkx\n",
    "    G_test = nx.relabel_nodes(G_test, {n: int(n) for n in G_test.nodes()})\n",
    "\n",
    "    num_nodes = len(G_test.nodes)\n",
    "    edge_index = torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    for modelo_name in resultados.keys():\n",
    "        for size in sizes:\n",
    "            if modelo_name in modelos_clasicos:\n",
    "                semillas = seleccionar_semillas_clasico(G_test, size, modelos_clasicos[modelo_name])\n",
    "            else:\n",
    "                modelo, model_type = modelos_gnn_ic[modelo_name]\n",
    "                data_test = Data(x=torch.tensor(np.random.rand(len(G_test.nodes()), 64), dtype=torch.float).to(device),\n",
    "                                 edge_index=torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous().to(device))\n",
    "                semillas = seleccionar_semillas(modelo, data_test, size, model_type)\n",
    "\n",
    "            infectados_total = 0\n",
    "            num_runs = 10\n",
    "            for _ in range(num_runs):\n",
    "                infectados = simular_difusion_IC(G_test, semillas)\n",
    "                infectados_total += infectados\n",
    "            influencia_promedio = infectados_total / num_runs / len(G_test.nodes())\n",
    "            resultados[modelo_name].append(influencia_promedio)\n",
    "\n",
    "    # Convertir los resultados a DataFrame y sumar al acumulado\n",
    "    df_resultados_temp = pd.DataFrame(resultados, index=[f'Size_{size}' for size in sizes]).T\n",
    "    if df_acumulado_IC_sinteticas.empty:\n",
    "        df_acumulado_IC_sinteticas = df_resultados_temp\n",
    "    else:\n",
    "        df_acumulado_IC_sinteticas += df_resultados_temp\n",
    "\n",
    "    # Generar gráfica para cada red\n",
    "    plot_influencia(resultados, list(sizes), f'Influencia Promedio en IC vs. Tamaño de Semillas Iniciales para Red {index}', index, 'IC')\n",
    "    plot_influencia_01(resultados, list(sizes), f'Influencia Promedio en IC vs. Tamaño de Semillas Iniciales para Red {index}', index, 'IC')\n",
    "\n",
    "    \n",
    "# Calcular el promedio de influencia para cada modelo y tamaño de semilla\n",
    "df_resultados_promedio_IC_sinteticas = df_acumulado_IC_sinteticas / 6\n",
    "\n",
    "# Guardar los resultados promedio\n",
    "df_resultados_promedio_IC_sinteticas.to_csv('./resultados_TFG/resultados_influencia_normalizada_promedio_IC_sinteticas.csv')     \n",
    "    \n",
    "\n",
    "    \n",
    "# Inicializar DataFrame para acumular resultados\n",
    "df_acumulado_IC = pd.DataFrame(columns=[f'Size_{size}' for size in sizes])     \n",
    "    \n",
    "# Proceso similar para IC\n",
    "for index in range(6, 14):\n",
    "    resultados = {modelo_name: [] for modelo_name in {**modelos_gnn_ic, **modelos_clasicos}}\n",
    "    G_test = redes_test[index]\n",
    "    G_test = to_networkx(G_test, to_undirected=True)\n",
    "    G_test = nx.relabel_nodes(G_test, {n: int(n) for n in G_test.nodes()})\n",
    "\n",
    "    for modelo_name in resultados.keys():\n",
    "        for size in sizes:\n",
    "            if modelo_name in modelos_clasicos:\n",
    "                semillas = seleccionar_semillas_clasico(G_test, size, modelos_clasicos[modelo_name])\n",
    "            else:\n",
    "                modelo, model_type = modelos_gnn_ic[modelo_name]\n",
    "                data_test = Data(x=torch.tensor(np.random.rand(len(G_test.nodes()), 64), dtype=torch.float).to(device),\n",
    "                                 edge_index=torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous().to(device))\n",
    "                semillas = seleccionar_semillas(modelo, data_test, size, model_type)\n",
    "\n",
    "            infectados_total = 0\n",
    "            num_runs = 10\n",
    "            for _ in range(num_runs):\n",
    "                infectados = simular_difusion_IC(G_test, semillas)\n",
    "                infectados_total += infectados\n",
    "            influencia_promedio = infectados_total / num_runs / len(G_test.nodes())\n",
    "            resultados[modelo_name].append(influencia_promedio)\n",
    "\n",
    "    # Convertir los resultados a DataFrame y sumar al acumulado\n",
    "    df_resultados_temp = pd.DataFrame(resultados, index=[f'Size_{size}' for size in sizes]).T\n",
    "    if df_acumulado_IC.empty:\n",
    "        df_acumulado_IC = df_resultados_temp\n",
    "    else:\n",
    "        df_acumulado_IC += df_resultados_temp\n",
    "\n",
    "    # Generar gráfica para cada red\n",
    "    plot_influencia(resultados, list(sizes), f'Influencia Promedio en IC vs. Tamaño de Semillas Iniciales para Red {index}', index, 'IC')\n",
    "    plot_influencia_01(resultados, list(sizes), f'Influencia Promedio en IC vs. Tamaño de Semillas Iniciales para Red {index}', index, 'IC')\n",
    "\n",
    "    \n",
    "# Calcular el promedio de influencia para cada modelo y tamaño de semilla\n",
    "df_resultados_promedio_IC = df_acumulado_IC / 8\n",
    "\n",
    "# Guardar los resultados promedio\n",
    "df_resultados_promedio_IC.to_csv('./resultados_TFG/resultados_influencia_normalizada_promedio_IC.csv')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyg_env)",
   "language": "python",
   "name": "pyg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
