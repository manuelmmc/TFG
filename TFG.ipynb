{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc81c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e2320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from matplotlib) (6.1.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manue\\anaconda3\\envs\\pyg_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69551171",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parámetros para las redes\n",
    "num_nodos = [1000, 3000]  # Número de nodos\n",
    "grados_medios = [2, 4, 8]  # Grado medio de cada nodo\n",
    "\n",
    "# Almacenar las redes generadas\n",
    "redes = []\n",
    "\n",
    "for n in num_nodos:\n",
    "    for k in grados_medios:\n",
    "        # El número de enlaces que se añaden desde un nuevo nodo a nodos existentes\n",
    "        m = k // 2  # Se divide por 2 porque en el modelo Barabasi-Albert cada enlace añade 2 al grado total\n",
    "        # Generar la red Barabasi-Albert\n",
    "        G = nx.barabasi_albert_graph(n, m)\n",
    "        redes.append(G)\n",
    "        # Opcional: Visualizar la red\n",
    "        #nx.draw(G, with_labels=False, node_size=30)\n",
    "        #plt.show()\n",
    "\n",
    "print(f\"Se han generado {len(redes)} redes Barabasi-Albert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9134f",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parámetros para las redes\n",
    "num_nodos = [2000, 4000]  # Número de nodos\n",
    "grados_medios = [2, 4, 8]  # Grado medio de cada nodo\n",
    "\n",
    "# Almacenar las redes generadas\n",
    "redes_test = []\n",
    "\n",
    "for n in num_nodos:\n",
    "    for k in grados_medios:\n",
    "        # El número de enlaces que se añaden desde un nuevo nodo a nodos existentes\n",
    "        m = k // 2  # Se divide por 2 porque en el modelo Barabasi-Albert cada enlace añade 2 al grado total\n",
    "        # Generar la red Barabasi-Albert\n",
    "        G = nx.barabasi_albert_graph(n, m)\n",
    "        redes_test.append(G)\n",
    "        # Opcional: Visualizar la red\n",
    "        #nx.draw(G, with_labels=False, node_size=30)\n",
    "        #plt.show()\n",
    "\n",
    "print(f\"Se han generado {len(redes_test)} redes Barabasi-Albert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85077d",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "\n",
    "def guardar_redes(redes, prefijo):\n",
    "    for i, G in enumerate(redes):\n",
    "        nx.write_adjlist(G, f\"{prefijo}_red_{i}.adjlist\")\n",
    "\n",
    "# Suponiendo que `redes` y `redes_test` son tus listas de redes\n",
    "guardar_redes(redes, \"train\")\n",
    "guardar_redes(redes_test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cdf9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def cargar_redes(prefijo, num_redes):\n",
    "    redes = []\n",
    "    for i in range(num_redes):\n",
    "        try:\n",
    "            path = f\"{prefijo}_red_{i}.adjlist\"\n",
    "            G = nx.read_adjlist(path)\n",
    "            redes.append(G)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No se encontró el archivo: {path}\")\n",
    "            break\n",
    "    return redes\n",
    "\n",
    "num_redes_train = 6  # Ajusta esto según el número de redes que generas\n",
    "num_redes_test = 6  # Ajusta esto según el número de redes de prueba que generas\n",
    "\n",
    "redes = cargar_redes(\"train\", num_redes_train)\n",
    "redes_test = cargar_redes(\"test\", num_redes_test)\n",
    "\n",
    "if not redes or not redes_test:\n",
    "    print(f\"hay algun problema.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4405cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def simulate_SIR_and_generate_labels(G, initial_infecteds, p=0.2, r=0.02, max_steps=100):\n",
    "    # Inicializar todos los nodos a susceptibles\n",
    "    status = {node: 'S' for node in G.nodes()}\n",
    "\n",
    "    # Infectar los nodos iniciales\n",
    "    for node in initial_infecteds:\n",
    "        status[node] = 'I'\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        new_status = dict(status)\n",
    "        for node in G.nodes():\n",
    "            if status[node] == 'I':\n",
    "                # Intentar infectar a los vecinos\n",
    "                for neighbor in G.neighbors(node):\n",
    "                    if status[neighbor] == 'S' and random.random() < p:\n",
    "                        new_status[neighbor] = 'I'\n",
    "                # Recuperar el nodo infectado\n",
    "                if random.random() < r:\n",
    "                    new_status[node] = 'R'\n",
    "        if status == new_status:\n",
    "            break  # Termina si no hay cambios\n",
    "        status = new_status\n",
    "\n",
    "    # Generar etiquetas basadas en el estado final de los nodos\n",
    "    labels = [1 if status[node] == 'I' else 0 for node in G.nodes()]\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f960b9",
   "metadata": {},
   "source": [
    "# Establecer el tamaño de nodos infectados iniciales a 30\n",
    "initial_infected_size = 30\n",
    "results_by_network_SIR = []\n",
    "\n",
    "for G in redes:\n",
    "    # Seleccionar nodos infectados iniciales basado en el tamaño de 30\n",
    "    initial_infecteds = random.sample(list(G.nodes()), initial_infected_size)\n",
    "    # Simular el modelo SIR\n",
    "    results = simulate_SIR_and_generate_labels(G, initial_infecteds)\n",
    "    # Guardar los resultados\n",
    "    results_by_network_SIR.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296ef985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_IC_and_generate_labels(G, initial_infecteds, p=0.2, max_steps=100):\n",
    "    # Inicializar todos los nodos como susceptibles excepto los infectados iniciales\n",
    "    status = {node: 'S' for node in G.nodes()}\n",
    "    for node in initial_infecteds:\n",
    "        status[node] = 'I'\n",
    "    \n",
    "    newly_infected = set(initial_infecteds)\n",
    "    for step in range(max_steps):\n",
    "        if not newly_infected:\n",
    "            break  # Terminar si no hay nuevos infectados\n",
    "        \n",
    "        new_status = dict(status)\n",
    "        current_newly_infected = set()\n",
    "        for infected_node in newly_infected:\n",
    "            for neighbor in G.neighbors(infected_node):\n",
    "                if status[neighbor] == 'S' and random.random() < p:\n",
    "                    new_status[neighbor] = 'I'\n",
    "                    current_newly_infected.add(neighbor)\n",
    "        \n",
    "        status = new_status\n",
    "        newly_infected = current_newly_infected\n",
    "        \n",
    "        if not newly_infected:\n",
    "            break  # Termina si no hay nuevos infectados en este paso\n",
    "    \n",
    "    # Generar etiquetas basadas en el estado final de los nodos\n",
    "    labels = [1 if status[node] == 'I' else 0 for node in G.nodes()]\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Ahora vamos a generar las etiquetas con el modelo IC para todas las redes\n",
    "# Establecer el tamaño de nodos infectados iniciales a 30\n",
    "#initial_infected_size = 30\n",
    "#results_by_network_IC = []\n",
    "\n",
    "#for G in redes:\n",
    "    # Seleccionar nodos infectados iniciales basado en el tamaño de 30\n",
    "    #initial_infecteds = random.sample(list(G.nodes()), initial_infected_size)\n",
    "    # Simular el modelo SIR\n",
    "    #results = simulate_IC_and_generate_labels(G, initial_infecteds)\n",
    "    # Guardar los resultados\n",
    "    #results_by_network_IC.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6952a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\manue\\\\struc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32387e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\manue\\\\struc2vec\\\\src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c984f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43f82a",
   "metadata": {},
   "source": [
    "from src.struc2vec import Graph\n",
    "import networkx as nx\n",
    "\n",
    "for i, G in enumerate(redes):\n",
    "    nx.write_edgelist(G, f\"red_sintetica_{i}.edgelist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995b18c",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "\n",
    "ruta_python_env = 'C:\\\\Users\\\\manue\\\\anaconda3\\\\envs\\\\pyg_env\\\\python.exe'\n",
    "ruta_main_struc2vec = 'C:\\\\Users\\\\manue\\\\struc2vec\\\\src\\\\main.py'  # Asumiendo que main.py está en el directorio src\n",
    "\n",
    "for i, _ in enumerate(redes):\n",
    "    comando = f\"{ruta_python_env} {ruta_main_struc2vec} --input red_sintetica_{i}.edgelist --output embeddings_{i}.emb --dimensions 128 --walk-length 80 --num-walks 10 --workers 8 --OPT1 True --OPT2 True --OPT3 True\"\n",
    "    resultado = subprocess.run(comando, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if resultado.returncode != 0:\n",
    "        print(f\"Error en la ejecución para la red {i}: {resultado.stderr}\")\n",
    "    else:\n",
    "        print(f\"Salida para la red {i}: {resultado.stdout}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2b1ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los embeddings para la red 0 existen.\n",
      "Los embeddings para la red 1 existen.\n",
      "Los embeddings para la red 2 existen.\n",
      "Los embeddings para la red 3 existen.\n",
      "Los embeddings para la red 4 existen.\n",
      "Los embeddings para la red 5 existen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i, _ in enumerate(redes):\n",
    "    if os.path.isfile(f\"embeddings_{i}.emb\"):\n",
    "        print(f\"Los embeddings para la red {i} existen.\")\n",
    "    else:\n",
    "        print(f\"No se encontraron embeddings para la red {i}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d3e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Abrir el archivo y leer las líneas, omitiendo la primera línea de cabecera\n",
    "with open('C:\\\\Users\\\\manue\\\\soc-wiki-Vote.mtx', 'r') as file:\n",
    "    data = file.readlines()[1:]  # Ignora la primera línea con el comentario del MatrixMarket\n",
    "\n",
    "# Preparar listas para filas y columnas\n",
    "rows, cols = [], []\n",
    "\n",
    "# Procesar cada línea excepto la cabecera\n",
    "for line in data:\n",
    "    if line.strip():  # asegurarse de que la línea no esté vacía\n",
    "        parts = line.split()\n",
    "        rows.append(int(parts[0]) - 1)  # Los índices en MTX están basados en 1, los convertimos a base 0\n",
    "        cols.append(int(parts[1]) - 1)\n",
    "\n",
    "# Crear un grafo en NetworkX\n",
    "G = nx.Graph()\n",
    "edges = zip(rows, cols)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Opcional: convertir a formato de PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80efa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Crear un grafo vacío en NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Leer las aristas desde el archivo 'fb-pages-food.edges'\n",
    "with open('fb-pages-food.edges', 'r') as file:\n",
    "    for line in file:\n",
    "        if not line.startswith('%'):  # Ignorar líneas de comentarios\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                source = int(parts[0])\n",
    "                target = int(parts[1])\n",
    "                G.add_edge(source, target)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc1433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Abrir el archivo y leer las líneas, omitiendo la primera línea de cabecera\n",
    "with open('C:\\\\Users\\\\manue\\\\web-edu.mtx', 'r') as file:\n",
    "    data = file.readlines()[1:]  # Ignora la primera línea con el comentario del MatrixMarket\n",
    "\n",
    "# Preparar listas para filas y columnas\n",
    "rows, cols = [], []\n",
    "\n",
    "# Procesar cada línea excepto la cabecera\n",
    "for line in data:\n",
    "    if line.strip():  # asegurarse de que la línea no esté vacía\n",
    "        parts = line.split()\n",
    "        rows.append(int(parts[0]) - 1)  # Los índices en MTX están basados en 1, los convertimos a base 0\n",
    "        cols.append(int(parts[1]) - 1)\n",
    "\n",
    "# Crear un grafo en NetworkX\n",
    "G = nx.Graph()\n",
    "edges = zip(rows, cols)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Opcional: convertir a formato de PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a33205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Abrir el archivo y leer las líneas, omitiendo la primera línea de cabecera\n",
    "with open('C:\\\\Users\\\\manue\\\\ia-fb-messages.mtx', 'r') as file:\n",
    "    data = file.readlines()[1:]  # Ignora la primera línea con el comentario del MatrixMarket\n",
    "\n",
    "# Preparar listas para filas y columnas\n",
    "rows, cols = [], []\n",
    "\n",
    "# Procesar cada línea excepto la cabecera\n",
    "for line in data:\n",
    "    if line.strip():  # asegurarse de que la línea no esté vacía\n",
    "        parts = line.split()\n",
    "        rows.append(int(parts[0]) - 1)  # Los índices en MTX están basados en 1, los convertimos a base 0\n",
    "        cols.append(int(parts[1]) - 1)\n",
    "\n",
    "# Crear un grafo en NetworkX\n",
    "G = nx.Graph()\n",
    "edges = zip(rows, cols)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Opcional: convertir a formato de PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb1f9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Nombre del archivo de texto\n",
    "archivo = 'USairport500.txt'\n",
    "\n",
    "# Crear un grafo en NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Abrir el archivo y leer las líneas\n",
    "with open(archivo, 'r') as file:\n",
    "    for line in file:\n",
    "        # Dividir la línea en sus componentes y convertirlos en enteros\n",
    "        source, target, weight = map(int, line.split())\n",
    "        # Agregar los nodos y las aristas al grafo\n",
    "        G.add_edge(source, target, weight=weight)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a4372e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Ruta al archivo que contiene las aristas\n",
    "archivo_aristas = 'soc-hamsterster.edges'\n",
    "\n",
    "# Crear un grafo vacío\n",
    "G = nx.Graph()\n",
    "\n",
    "# Leer el archivo de aristas y agregarlas al grafo\n",
    "with open(archivo_aristas, 'r') as file:\n",
    "    for line in file:\n",
    "        if not line.startswith('%'):  # Ignorar líneas de comentarios\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                source = int(parts[0])\n",
    "                target = int(parts[1])\n",
    "                G.add_edge(source, target)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21185611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Crear un grafo vacío en NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Leer el archivo CSV y agregar las aristas al grafo\n",
    "with open('soc-sign-bitcoinalpha.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(',')\n",
    "        source = int(parts[0])\n",
    "        target = int(parts[1])\n",
    "        G.add_edge(source, target)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b978a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from scipy.io import mmread\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Leer el archivo MTX\n",
    "matrix = mmread('econ-mahindas.mtx')\n",
    "\n",
    "# Convertir la matriz dispersa en un grafo NetworkX\n",
    "G = nx.Graph(matrix)\n",
    "\n",
    "# Convertir el grafo a un objeto PyTorch Geometric\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Agregar el nuevo grafo a la lista de redes de prueba\n",
    "redes_test.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65609fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de redes en redes_test: 14\n"
     ]
    }
   ],
   "source": [
    "num_redes = len(redes_test)\n",
    "print(\"Número de redes en redes_test:\", num_redes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5bbe4",
   "metadata": {},
   "source": [
    "from src.struc2vec import Graph\n",
    "import networkx as nx\n",
    "\n",
    "for i, G in enumerate(redes_test):\n",
    "    nx.write_edgelist(G, f\"red_sintetica_{i}_test.edgelist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c2147",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "\n",
    "ruta_python_env = 'C:\\\\Users\\\\manue\\\\anaconda3\\\\envs\\\\pyg_env\\\\python.exe'\n",
    "ruta_main_struc2vec = 'C:\\\\Users\\\\manue\\\\struc2vec\\\\src\\\\main.py'  # Asumiendo que main.py está en el directorio src\n",
    "\n",
    "# Ejecuta struc2vec solo para las primeras 6 redes\n",
    "for i in range(6):\n",
    "    comando = f\"{ruta_python_env} {ruta_main_struc2vec} --input red_sintetica_{i}_test.edgelist --output embeddings_{i}_test.emb --dimensions 128 --walk-length 80 --num-walks 10 --workers 8 --OPT1 True --OPT2 True --OPT3 True\"\n",
    "    resultado = subprocess.run(comando, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if resultado.returncode != 0:\n",
    "        print(f\"Error en la ejecución para la red {i}: {resultado.stderr}\")\n",
    "    else:\n",
    "        print(f\"Salida para la red {i}: {resultado.stdout}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39da4d8",
   "metadata": {},
   "source": [
    "from src.struc2vec import Graph\n",
    "import networkx as nx\n",
    "import subprocess\n",
    "\n",
    "ruta_python_env = 'C:\\\\Users\\\\manue\\\\anaconda3\\\\envs\\\\pyg_env\\\\python.exe'\n",
    "ruta_main_struc2vec = 'C:\\\\Users\\\\manue\\\\struc2vec\\\\src\\\\main.py'\n",
    "\n",
    "def save_edgelist(graph, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for edge in graph.edge_index.T.tolist():\n",
    "            file.write(f\"{edge[0]} {edge[1]}\\n\")\n",
    "\n",
    "indice_inicio = len(redes_test) - 8\n",
    "\n",
    "for i in range(indice_inicio, len(redes_test)):\n",
    "    save_edgelist(redes_test[i], f\"red_sintetica_{i}_test.edgelist\")\n",
    "\n",
    "\n",
    "for i in range(indice_inicio, len(redes_test)):    \n",
    "    comando = f\"{ruta_python_env} {ruta_main_struc2vec} --input red_sintetica_{i}_test.edgelist --output embeddings_{i}_test.emb --dimensions 128 --walk-length 80 --num-walks 10 --workers 8 --OPT1 True --OPT2 True --OPT3 True\"\n",
    "    resultado = subprocess.run(comando, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if resultado.returncode != 0:\n",
    "        print(f\"Error en la ejecución para la red {i}: {resultado.stderr}\")\n",
    "    else:\n",
    "        print(f\"Salida para la red {i}: {resultado.stdout}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f12f95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.struc2vec import Graph\n",
    "#import networkx as nx\n",
    "#import subprocess\n",
    "\n",
    "#ruta_python_env = 'C:\\\\Users\\\\manue\\\\anaconda3\\\\envs\\\\pyg_env\\\\python.exe'\n",
    "#ruta_main_struc2vec = 'C:\\\\Users\\\\manue\\\\struc2vec\\\\src\\\\main.py'\n",
    "\n",
    "#def save_edgelist(graph, filename):\n",
    "#    with open(filename, 'w') as file:\n",
    "#        for edge in graph.edge_index.T.tolist():\n",
    " #           file.write(f\"{edge[0]} {edge[1]}\\n\")\n",
    "\n",
    "## Índice de la red 7 en la lista redes_test\n",
    "#indice_red_7 = 7\n",
    "\n",
    "## Guardar la lista de adyacencia de la red 7\n",
    "#save_edgelist(redes_test[indice_red_7], f\"red_sintetica_{indice_red_7}_test.edgelist\")\n",
    "\n",
    "## Ejecutar struc2vec solo para la red 7\n",
    "#comando = f\"{ruta_python_env} {ruta_main_struc2vec} --input red_sintetica_{indice_red_7}_test.edgelist --output embeddings_{indice_red_7}_test.emb --dimensions 128 --walk-length 80 --num-walks 10 --workers 8 --OPT1 True --OPT2 True --OPT3 True\"\n",
    "#resultado = subprocess.run(comando, shell=True, capture_output=True, text=True)\n",
    "\n",
    "#if resultado.returncode != 0:\n",
    "#    print(f\"Error en la ejecución para la red {indice_red_7}: {resultado.stderr}\")\n",
    "#else:\n",
    "#    print(f\"Salida para la red {indice_red_7}: {resultado.stdout}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e910b795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los embeddings para la red 0 existen.\n",
      "Los embeddings para la red 1 existen.\n",
      "Los embeddings para la red 2 existen.\n",
      "Los embeddings para la red 3 existen.\n",
      "Los embeddings para la red 4 existen.\n",
      "Los embeddings para la red 5 existen.\n",
      "Los embeddings para la red 6 existen.\n",
      "Los embeddings para la red 7 existen.\n",
      "Los embeddings para la red 8 existen.\n",
      "Los embeddings para la red 9 existen.\n",
      "Los embeddings para la red 10 existen.\n",
      "Los embeddings para la red 11 existen.\n",
      "Los embeddings para la red 12 existen.\n",
      "Los embeddings para la red 13 existen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i, _ in enumerate(redes_test):\n",
    "    if os.path.isfile(f\"embeddings_{i}_test.emb\"):\n",
    "        print(f\"Los embeddings para la red {i} existen.\")\n",
    "    else:\n",
    "        print(f\"No se encontraron embeddings para la red {i}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e098ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "data_list_SIR = []  # Para almacenar objetos Data de todas las redes\n",
    "\n",
    "for i in range(len(redes)):  # Asumimos que 'redes' contiene todas tus redes\n",
    "    # Cargar la red sintética\n",
    "    G = nx.read_edgelist(f'red_sintetica_{i}.edgelist', nodetype=int)\n",
    "    \n",
    "    # Cargar los embeddings generados por struc2vec\n",
    "    embeddings = np.loadtxt(f'embeddings_{i}.emb', skiprows=1)\n",
    "    node_indices = embeddings[:, 0].astype(int)  # No se usa directamente, pero útil para validar\n",
    "    node_features = embeddings[:, 1:]  # Las características (embeddings) de los nodos\n",
    "    \n",
    "    # Crear un tensor de características\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Crear listas de aristas\n",
    "    edge_list = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Generar etiquetas usando la función de simulación SIR o IC modificada\n",
    "    # Aquí usamos simulate_SIR_and_generate_labels como ejemplo\n",
    "    initial_infecteds = random.sample(list(G.nodes()), 30)  # Escoger 30 nodos al azar como infectados iniciales\n",
    "    etiquetas = simulate_SIR_and_generate_labels(G, initial_infecteds)\n",
    "    \n",
    "    y = torch.tensor(etiquetas, dtype=torch.long)\n",
    "    \n",
    "    # Crear el objeto Data con x, edge_index y y\n",
    "    data = Data(x=x, edge_index=edge_list, y=y)\n",
    "    \n",
    "    # Opcional: Crear y asignar train_mask aquí si es necesario\n",
    "    # train_mask = torch.rand(len(etiquetas)) < 0.8  # Ejemplo de cómo crear un train_mask\n",
    "    # data.train_mask = train_mask\n",
    "    \n",
    "    data_list_SIR.append(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01f36876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "data_list_IC = []  # Para almacenar objetos Data de todas las redes\n",
    "\n",
    "for i in range(len(redes)):  # Asumimos que 'redes' contiene todas tus redes\n",
    "    # Cargar la red sintética\n",
    "    G = nx.read_edgelist(f'red_sintetica_{i}.edgelist', nodetype=int)\n",
    "    \n",
    "    # Cargar los embeddings generados por struc2vec\n",
    "    embeddings = np.loadtxt(f'embeddings_{i}.emb', skiprows=1)\n",
    "    node_indices = embeddings[:, 0].astype(int)  # No se usa directamente, pero útil para validar\n",
    "    node_features = embeddings[:, 1:]  # Las características (embeddings) de los nodos\n",
    "    \n",
    "    # Crear un tensor de características\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Crear listas de aristas\n",
    "    edge_list = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Generar etiquetas usando la función de simulación SIR o IC modificada\n",
    "    # Aquí usamos simulate_SIR_and_generate_labels como ejemplo\n",
    "    initial_infecteds = random.sample(list(G.nodes()), 30)  # Escoger 30 nodos al azar como infectados iniciales\n",
    "    etiquetas = simulate_IC_and_generate_labels(G, initial_infecteds)\n",
    "    \n",
    "    y = torch.tensor(etiquetas, dtype=torch.long)\n",
    "    \n",
    "    # Crear el objeto Data con x, edge_index y y\n",
    "    data = Data(x=x, edge_index=edge_list, y=y)\n",
    "    \n",
    "    # Opcional: Crear y asignar train_mask aquí si es necesario\n",
    "    # train_mask = torch.rand(len(etiquetas)) < 0.8  # Ejemplo de cómo crear un train_mask\n",
    "    # data.train_mask = train_mask\n",
    "    \n",
    "    data_list_IC.append(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53fb97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "from torch.nn import Dropout  # Corregido\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.bn1 = BatchNorm(hidden_dim)  # Batch Normalization después de la primera convolución\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)  # Añadimos una capa convolucional adicional\n",
    "        self.bn2 = BatchNorm(hidden_dim)  # Batch Normalization después de la segunda convolución\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn3 = BatchNorm(hidden_dim)  # Batch Normalization después de la segunda convolución\n",
    "        self.conv4 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(0.4)  # Reducimos el Dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)  # Aplicamos Batch Normalization\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)  # Aplicamos Dropout\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)  # Aplicamos Batch Normalization\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)  # Aplicamos Dropout\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)  # Aplicamos Batch Normalization\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)  # Aplicamos Dropout\n",
    "        x = self.conv4(x, edge_index)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98fcbdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto 1: Training Loss: 0.40359067916870117, Validation Loss: 0.36376675963401794\n",
      "Modelo SIR 1 guardado en model_SIR_0.pt\n",
      "Conjunto 2: Training Loss: 0.3915309011936188, Validation Loss: 0.4695465564727783\n",
      "Modelo SIR 2 guardado en model_SIR_1.pt\n",
      "Conjunto 3: Training Loss: 0.43096596002578735, Validation Loss: 0.4666045010089874\n",
      "Modelo SIR 3 guardado en model_SIR_2.pt\n",
      "Conjunto 4: Training Loss: 0.4799681305885315, Validation Loss: 0.4816197156906128\n",
      "Modelo SIR 4 guardado en model_SIR_3.pt\n",
      "Conjunto 5: Training Loss: 0.44698747992515564, Validation Loss: 0.4278711676597595\n",
      "Modelo SIR 5 guardado en model_SIR_4.pt\n",
      "Conjunto 6: Training Loss: 0.4273779094219208, Validation Loss: 0.40618520975112915\n",
      "Modelo SIR 6 guardado en model_SIR_5.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "modelos_paths_GCN_SIR = []  # Lista para almacenar las rutas de los archivos de los modelos guardados\n",
    "\n",
    "for idx, data in enumerate(data_list_SIR):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GNN(input_dim=x.size(1), hidden_dim=64, output_dim=2).to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.rand(num_nodes) < 0.8\n",
    "    val_mask = ~train_mask\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    \n",
    "     # Funciones para entrenar y validar\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def validate():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "        return val_loss\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        train_loss = train()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            val_loss = validate()\n",
    "            if epoch == 999:  # Guarda los resultados en la última época\n",
    "                print(f'Conjunto {idx+1}: Training Loss: {train_loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "    \n",
    "    # Guardar el modelo después del entrenamiento\n",
    "    model_path = f'model_SIR_{idx}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    modelos_paths_GCN_SIR.append(model_path)\n",
    "    print(f'Modelo SIR {idx+1} guardado en {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ef7affe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto 1: Training Loss: 0.188833549618721, Validation Loss: 0.1882631778717041\n",
      "Modelo IC 1 guardado en model_IC_0.pt\n",
      "Conjunto 2: Training Loss: 0.5829439759254456, Validation Loss: 0.5624985694885254\n",
      "Modelo IC 2 guardado en model_IC_1.pt\n",
      "Conjunto 3: Training Loss: 0.6503745317459106, Validation Loss: 0.6663835644721985\n",
      "Modelo IC 3 guardado en model_IC_2.pt\n",
      "Conjunto 4: Training Loss: 0.1422957181930542, Validation Loss: 0.16148650646209717\n",
      "Modelo IC 4 guardado en model_IC_3.pt\n",
      "Conjunto 5: Training Loss: 0.48393163084983826, Validation Loss: 0.5227402448654175\n",
      "Modelo IC 5 guardado en model_IC_4.pt\n",
      "Conjunto 6: Training Loss: 0.6710789203643799, Validation Loss: 0.6695987582206726\n",
      "Modelo IC 6 guardado en model_IC_5.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "modelos_paths_GCN_IC = []  # Lista para almacenar las rutas de los archivos de los modelos guardados\n",
    "\n",
    "for idx, data in enumerate(data_list_IC):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GNN(input_dim=x.size(1), hidden_dim=64, output_dim=2).to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.rand(num_nodes) < 0.8\n",
    "    val_mask = ~train_mask\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    \n",
    "     # Funciones para entrenar y validar\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def validate():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "        return val_loss\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        train_loss = train()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            val_loss = validate()\n",
    "            if epoch == 999:  # Guarda los resultados en la última época\n",
    "                print(f'Conjunto {idx+1}: Training Loss: {train_loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "    \n",
    "    # Guardar el modelo después del entrenamiento\n",
    "    model_path = f'model_IC_{idx}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    modelos_paths_GCN_IC.append(model_path)\n",
    "    print(f'Modelo IC {idx+1} guardado en {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26d8a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def simular_difusion_SIR(G, semillas, p=0.05, r=0.02, max_steps=100):\n",
    "    status = {node: 'S' for node in G.nodes()}\n",
    "    for node in semillas:\n",
    "        status[node] = 'I'\n",
    "    \n",
    "    #print(f\"Estado inicial (semillas infectadas): {semillas}\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        new_status = dict(status)\n",
    "        infectados = 0\n",
    "        recuperados = 0\n",
    "        for node in G.nodes():\n",
    "            if status[node] == 'I':\n",
    "                for neighbor in G.neighbors(node):\n",
    "                    if status[neighbor] == 'S' and random.random() < p:\n",
    "                        new_status[neighbor] = 'I'\n",
    "                        infectados += 1\n",
    "                if random.random() < r:\n",
    "                    new_status[node] = 'R'\n",
    "                    recuperados += 1\n",
    "        status = new_status\n",
    "        #print(f\"Paso {step + 1}: Infectados nuevos = {infectados}, Recuperados = {recuperados}\")\n",
    "        \n",
    "        if infectados == 0 and recuperados == 0:\n",
    "            #print(\"No hay más cambios en el estado de los nodos.\")\n",
    "            break\n",
    "    \n",
    "    total_infectados = sum(1 for state in status.values() if state in ['I', 'R'])\n",
    "    #print(f\"Total infectados al final: {total_infectados}\")\n",
    "    return total_infectados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "858e8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simular_difusion_IC(G, semillas, p=0.05, max_steps=100):\n",
    "    status = {node: 'S' for node in G.nodes()}\n",
    "    for node in semillas:\n",
    "        if node not in G:\n",
    "            print(f\"El nodo {node} no está en el grafo.\")\n",
    "        status[node] = 'I'\n",
    "    \n",
    "    newly_infected = set(semillas)\n",
    "    #print(f\"Estado inicial (semillas infectadas): {semillas}\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if not newly_infected:\n",
    "     #       print(\"No hay nuevos infectados.\")\n",
    "            break\n",
    "        \n",
    "        new_status = dict(status)\n",
    "        current_newly_infected = set()\n",
    "        for infected_node in newly_infected:\n",
    "            if infected_node not in G:\n",
    "                continue  # Salta a la siguiente iteración si el nodo no existe\n",
    "            for neighbor in G.neighbors(infected_node):\n",
    "                if status[neighbor] == 'S' and np.random.random() < p:\n",
    "                    new_status[neighbor] = 'I'\n",
    "                    current_newly_infected.add(neighbor)\n",
    "        \n",
    "        status = new_status\n",
    "        newly_infected = current_newly_infected\n",
    "        #print(f\"Paso {step + 1}: Nuevos infectados = {len(newly_infected)}\")\n",
    "        \n",
    "    total_infectados = sum(1 for state in status.values() if state == 'I')\n",
    "    #print(f\"Total infectados al final: {total_infectados}\")\n",
    "    return total_infectados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b779d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_GCN_IC(i, input_dim, hidden_dim, output_dim, device):\n",
    "    model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    model.load_state_dict(torch.load(modelos_paths_GCN_IC[i]))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GCN_SIR(i, input_dim, hidden_dim, output_dim, device):\n",
    "    model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    model.load_state_dict(torch.load(modelos_paths_GCN_SIR[i]))\n",
    "    model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01db41f",
   "metadata": {},
   "source": [
    "def cargar_modelo_SIR(i, input_dim, hidden_dim, output_dim, device):\n",
    "    model_path = modelos_paths_SIR[i]\n",
    "    print(f\"Cargando modelo SIR desde: {model_path}\")  # Imprime la ruta del modelo\n",
    "    model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Imprimir detalles del modelo\n",
    "    print(\"Detalles del modelo SIR cargado:\")\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_IC(i, input_dim, hidden_dim, output_dim, device):\n",
    "    model_path = modelos_paths_IC[i]\n",
    "    print(f\"Cargando modelo IC desde: {model_path}\")  # Imprime la ruta del modelo\n",
    "    model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Imprimir detalles del modelo\n",
    "    print(\"Detalles del modelo IC cargado:\")\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "modelo_sir = cargar_modelo_SIR(0, input_dim=x.size(1), hidden_dim=64, output_dim=2, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "modelo_ic = cargar_modelo_IC(0, input_dim=x.size(1), hidden_dim=64, output_dim=2, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Ahora imprime el state_dict para verificar nombres de las capas\n",
    "print(\"Keys en el modelo SIR:\")\n",
    "print(list(modelo_sir.state_dict().keys()))\n",
    "print(\"Keys en el modelo IC:\")\n",
    "print(list(modelo_ic.state_dict().keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5c071",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Supongamos que 'G' es uno de tus grafos\n",
    "G = redes_test[5]  # Usando el primer grafo como ejemplo\n",
    "\n",
    "initial_infected_size = 30\n",
    "initial_infecteds = random.sample(list(G.nodes()), initial_infected_size)\n",
    "\n",
    "# Suponiendo que tienes funciones `simulate_SIR_and_generate_labels` y `simulate_IC_and_generate_labels` definidas\n",
    "labels_SIR = simular_difusion_SIR(G, initial_infecteds)\n",
    "labels_IC = simular_difusion_IC(G, initial_infecteds)\n",
    "\n",
    "# Comparar las etiquetas generadas por ambos modelos\n",
    "different_labels_count = np.sum(np.array(labels_SIR) != np.array(labels_IC))\n",
    "print(\"Número de etiquetas diferentes entre SIR e IC:\", different_labels_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d321dd3",
   "metadata": {},
   "source": [
    "# Asumiendo que el modelo y los datos están definidos en alguna parte del código\n",
    "# Esto simplemente imprime un subconjunto de los datos usados para entrenar, ajusta según tu estructura de datos\n",
    "\n",
    "print(\"Datos usados para entrenar el modelo SIR (primeras 10 etiquetas):\", labels_SIR[:10])\n",
    "print(\"Datos usados para entrenar el modelo IC (primeras 10 etiquetas):\", labels_IC[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a656d5f",
   "metadata": {},
   "source": [
    "# Cargar los modelos\n",
    "modelo_sir = cargar_modelo_SIR(0, input_dim=x.size(1), hidden_dim=64, output_dim=2, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "modelo_ic = cargar_modelo_IC(0, input_dim=x.size(1), hidden_dim=64, output_dim=2, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Comparar los pesos de alguna capa después del entrenamiento\n",
    "weights_SIR = modelo_sir.state_dict()['conv1.lin.weight'].cpu().numpy()\n",
    "weights_IC = modelo_ic.state_dict()['conv1.lin.weight'].cpu().numpy()\n",
    "print(\"Diferencia en pesos entre modelos SIR e IC:\", np.linalg.norm(weights_SIR - weights_IC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4375d65",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dibujar_grafo(grafo, title=\"Grafo\", filename=\"grafo.png\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pos = nx.spring_layout(grafo)\n",
    "    nx.draw(grafo, pos, node_color='lightblue', with_labels=True, node_size=10, font_size=10, edge_color='gray')\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "for i, grafo in enumerate(redes):\n",
    "    dibujar_grafo(grafo, f\"Grafo de Entrenamiento {i}\", f\"entrenamiento_{i}.png\")\n",
    "\n",
    "for i, grafo in enumerate(redes_test):\n",
    "    dibujar_grafo(grafo, f\"Grafo de Prueba {i}\", f\"prueba_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064edaff",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def dibujar_grafo_pyg(data, title=\"Grafo\", filename=None):\n",
    "    G = to_networkx(data, to_undirected=True)  # Convierte a grafo no dirigido\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pos = nx.spring_layout(G)  # Genera una disposición de los nodos del grafo para una visualización agradable\n",
    "    nx.draw(G, pos, node_color='lightblue', with_labels=True, node_size=500, font_size=10, edge_color='gray')\n",
    "    plt.title(title)\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "indice_inicio = 6\n",
    "# Si tienes los objetos Data guardados en listas, por ejemplo `redes_test_pyg`\n",
    "for i in range(indice_inicio, len(redes_test)):\n",
    "    data = redes_test[i]\n",
    "    dibujar_grafo_pyg(data, f\"Grafo de Prueba PyG {i}\", f\"prueba_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aadbd5",
   "metadata": {},
   "source": [
    "def comparar_modelos(modelo_sir, modelo_ic):\n",
    "    print(\"Comparando los modelos SIR e IC...\")\n",
    "    if modelo_sir.state_dict()['conv1.weight'].equal(modelo_ic.state_dict()['conv1.weight']):\n",
    "        print(\"Los pesos de la primera capa convolucional son iguales.\")\n",
    "    else:\n",
    "        print(\"Los pesos de la primera capa convolucional son diferentes.\")\n",
    "\n",
    "modelo_sir = cargar_modelo_SIR(0, input_dim=x.size(1), hidden_dim=64, output_dim=2, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "modelo_ic = cargar_modelo_IC(0, input_dim=x.size(1), hidden_dim=64, output_dim=2, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "comparar_modelos(modelo_sir, modelo_ic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dd8f5",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def check_and_convert_graph(graph, identifier=\"\"):\n",
    "    if isinstance(graph, Data):\n",
    "        # Convertir de PyTorch Geometric Data a NetworkX Graph\n",
    "        nx_graph = to_networkx(graph, to_undirected=True)\n",
    "        print(f\"Grafo {identifier} convertido de PyTorch Geometric a NetworkX.\")\n",
    "        return nx_graph\n",
    "    elif isinstance(graph, nx.Graph):\n",
    "        print(f\"Grafo {identifier} ya es un objeto NetworkX.\")\n",
    "        return graph\n",
    "    else:\n",
    "        print(f\"Error: El objeto {identifier} no es un grafo.\")\n",
    "        return None\n",
    "\n",
    "for i, G_test in enumerate(redes_test):\n",
    "    G_test = check_and_convert_graph(G_test, f\"Test Grafo {i}\")\n",
    "    if G_test is None or len(G_test.edges()) == 0:\n",
    "        print(f\"No hay aristas válidas para el Grafo {i}\")\n",
    "        continue  # Salta este grafo si no tiene aristas\n",
    "\n",
    "    # Procesamiento adicional aquí...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b968c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor red de entrenamiento SIR es la número 5, con un promedio de influencia normalizada de 7.523310964459225\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\" Verifica si el objeto es un string que representa un dígito. \"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "modelos_entrenados_SIR = [cargar_modelo_GCN_SIR(i, input_dim=128, hidden_dim=64, output_dim=2, device=device) for i in range(len(modelos_paths_GCN_SIR))]\n",
    "\n",
    "resultados_influencia_SIR = np.zeros((len(modelos_entrenados_SIR), len(redes_test)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_SIR):\n",
    "    for j, data_test in enumerate(redes_test):\n",
    "        if isinstance(data_test, Data):\n",
    "            G_test = to_networkx(data_test, to_undirected=True)\n",
    "        elif isinstance(data_test, nx.Graph):\n",
    "            G_test = data_test\n",
    "        else:\n",
    "            print(f\"Error: Grafo {j} no es compatible.\")\n",
    "            continue\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'embeddings_{j}_test.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_SIR[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_SIR = resultados_influencia_SIR.mean(axis=1)\n",
    "mejor_red_idx_GCN_SIR = np.argmax(promedio_influencia_por_red_SIR)\n",
    "print(f\"La mejor red de entrenamiento SIR es la número {mejor_red_idx_GCN_SIR + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_SIR[mejor_red_idx_GCN_SIR]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fb20a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor red de entrenamiento IC es la número 2, con un promedio de influencia normalizada de 0.9025205877423507\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\" Verifica si el objeto es un string que representa un dígito. \"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "modelos_entrenados_IC = [cargar_modelo_GCN_IC(i, input_dim=128, hidden_dim=64, output_dim=2, device=device) for i in range(len(modelos_paths_GCN_IC))]\n",
    "\n",
    "resultados_influencia_IC = np.zeros((len(modelos_entrenados_IC), len(redes_test)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_IC):\n",
    "    for j, data_test in enumerate(redes_test):\n",
    "        if isinstance(data_test, Data):\n",
    "            G_test = to_networkx(data_test, to_undirected=True)\n",
    "        elif isinstance(data_test, nx.Graph):\n",
    "            G_test = data_test\n",
    "        else:\n",
    "            print(f\"Error: Grafo {j} no es compatible.\")\n",
    "            continue\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'embeddings_{j}_test.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_IC(G_test, semillas)\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_IC[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_IC = resultados_influencia_IC.mean(axis=1)\n",
    "mejor_red_idx_GCN_IC = np.argmax(promedio_influencia_por_red_IC)\n",
    "print(f\"La mejor red de entrenamiento IC es la número {mejor_red_idx_GCN_IC + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_IC[mejor_red_idx_GCN_IC]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4029b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(in_features, out_features) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_features = out_features\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Concatenar las salidas de todas las cabezas de atención\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=1)\n",
    "        # Aquí deberías añadir tu mecanismo de atención real usando edge_index\n",
    "        return x\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat_layer = GATLayer(in_features, out_features, num_heads)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat_layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Ajusta 'in_features' a 128, que es la dimensión de tus características de entrada\n",
    "model = GAT(in_features=128, out_features=32, num_heads=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c90843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc48b1c",
   "metadata": {},
   "source": [
    "for idx, data in enumerate(data_list_SIR):\n",
    "    edge_indices = data.edge_index.cpu().numpy()  # Asegúrate de trabajar en la CPU\n",
    "    max_index = edge_indices.max()\n",
    "    min_index = edge_indices.min()\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    print(f\"Data {idx}:\")\n",
    "    print(f\"Shape of edge_index: {data.edge_index.shape}\")\n",
    "    print(f\"Max index in edge_index: {max_index}\")\n",
    "    print(f\"Min index in edge_index: {min_index}\")\n",
    "    print(f\"Number of nodes (should be greater than max index): {num_nodes}\")\n",
    "\n",
    "    if max_index >= num_nodes:\n",
    "        print(\"Error: Indices in edge_index are out of bounds!\")\n",
    "    if min_index < 0:\n",
    "        print(\"Error: Negative indices found in edge_index!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194db36",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Guarda el modelo cuando la pérdida de validación disminuya.'''\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9b455ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss: 0.42584773898124695, Val Loss: 0.3795829713344574\n",
      "Epoch 200: Train Loss: 0.3892073929309845, Val Loss: 0.3596489429473877\n",
      "Epoch 300: Train Loss: 0.37471500039100647, Val Loss: 0.36008384823799133\n",
      "Epoch 400: Train Loss: 0.3656540811061859, Val Loss: 0.3611381947994232\n",
      "Epoch 500: Train Loss: 0.3591890335083008, Val Loss: 0.36167553067207336\n",
      "Epoch 600: Train Loss: 0.3543430268764496, Val Loss: 0.36209040880203247\n",
      "Epoch 700: Train Loss: 0.35061511397361755, Val Loss: 0.36257439851760864\n",
      "Epoch 800: Train Loss: 0.3476964831352234, Val Loss: 0.3631610572338104\n",
      "Epoch 900: Train Loss: 0.3453789949417114, Val Loss: 0.3638227581977844\n",
      "Epoch 1000: Train Loss: 0.34351590275764465, Val Loss: 0.3645162582397461\n",
      "Best Val Loss: 0.35935071110725403\n",
      "Modelo GAT SIR 1 guardado en model_GAT_SIR_0.pt\n",
      "Epoch 100: Train Loss: 0.3892563581466675, Val Loss: 0.42190051078796387\n",
      "Epoch 200: Train Loss: 0.3764994442462921, Val Loss: 0.4297636151313782\n",
      "Epoch 300: Train Loss: 0.3696843087673187, Val Loss: 0.4329794943332672\n",
      "Epoch 400: Train Loss: 0.3656120300292969, Val Loss: 0.4353436231613159\n",
      "Epoch 500: Train Loss: 0.3629094958305359, Val Loss: 0.43739309906959534\n",
      "Epoch 600: Train Loss: 0.3610004186630249, Val Loss: 0.43920668959617615\n",
      "Epoch 700: Train Loss: 0.35958975553512573, Val Loss: 0.4407924711704254\n",
      "Epoch 800: Train Loss: 0.3585094213485718, Val Loss: 0.442164808511734\n",
      "Epoch 900: Train Loss: 0.3576560914516449, Val Loss: 0.44335034489631653\n",
      "Epoch 1000: Train Loss: 0.3569628894329071, Val Loss: 0.4443807303905487\n",
      "Best Val Loss: 0.39207831025123596\n",
      "Modelo GAT SIR 2 guardado en model_GAT_SIR_1.pt\n",
      "Epoch 100: Train Loss: 0.37959495186805725, Val Loss: 0.6237063407897949\n",
      "Epoch 200: Train Loss: 0.3658958375453949, Val Loss: 0.6323623657226562\n",
      "Epoch 300: Train Loss: 0.3589284420013428, Val Loss: 0.6384113430976868\n",
      "Epoch 400: Train Loss: 0.35470250248908997, Val Loss: 0.642028272151947\n",
      "Epoch 500: Train Loss: 0.351863831281662, Val Loss: 0.6442061066627502\n",
      "Epoch 600: Train Loss: 0.3498353064060211, Val Loss: 0.6455839276313782\n",
      "Epoch 700: Train Loss: 0.34832167625427246, Val Loss: 0.6465145349502563\n",
      "Epoch 800: Train Loss: 0.34715330600738525, Val Loss: 0.6471909880638123\n",
      "Epoch 900: Train Loss: 0.3462253212928772, Val Loss: 0.6477210521697998\n",
      "Epoch 1000: Train Loss: 0.3454694449901581, Val Loss: 0.6481640338897705\n",
      "Best Val Loss: 0.6183086633682251\n",
      "Modelo GAT SIR 3 guardado en model_GAT_SIR_2.pt\n",
      "Epoch 100: Train Loss: 0.4621678590774536, Val Loss: 0.5612998008728027\n",
      "Epoch 200: Train Loss: 0.4456380605697632, Val Loss: 0.5491517186164856\n",
      "Epoch 300: Train Loss: 0.4399646520614624, Val Loss: 0.546420693397522\n",
      "Epoch 400: Train Loss: 0.43737542629241943, Val Loss: 0.545499861240387\n",
      "Epoch 500: Train Loss: 0.4360155165195465, Val Loss: 0.5450475811958313\n",
      "Epoch 600: Train Loss: 0.4352293014526367, Val Loss: 0.5447608828544617\n",
      "Epoch 700: Train Loss: 0.43473953008651733, Val Loss: 0.544547438621521\n",
      "Epoch 800: Train Loss: 0.43441498279571533, Val Loss: 0.5443674325942993\n",
      "Epoch 900: Train Loss: 0.43418797850608826, Val Loss: 0.5442003607749939\n",
      "Epoch 1000: Train Loss: 0.4340212047100067, Val Loss: 0.5440368056297302\n",
      "Best Val Loss: 0.5440368056297302\n",
      "Modelo GAT SIR 4 guardado en model_GAT_SIR_3.pt\n",
      "Epoch 100: Train Loss: 0.4220046401023865, Val Loss: 0.4777442514896393\n",
      "Epoch 200: Train Loss: 0.4128705561161041, Val Loss: 0.47934794425964355\n",
      "Epoch 300: Train Loss: 0.4088491201400757, Val Loss: 0.480370432138443\n",
      "Epoch 400: Train Loss: 0.40664827823638916, Val Loss: 0.48090335726737976\n",
      "Epoch 500: Train Loss: 0.4052882194519043, Val Loss: 0.481155663728714\n",
      "Epoch 600: Train Loss: 0.4043791890144348, Val Loss: 0.4812696874141693\n",
      "Epoch 700: Train Loss: 0.40373772382736206, Val Loss: 0.4813203811645508\n",
      "Epoch 800: Train Loss: 0.4032666087150574, Val Loss: 0.4813433885574341\n",
      "Epoch 900: Train Loss: 0.4029097855091095, Val Loss: 0.48135489225387573\n",
      "Epoch 1000: Train Loss: 0.4026322066783905, Val Loss: 0.4813618063926697\n",
      "Best Val Loss: 0.4710777699947357\n",
      "Modelo GAT SIR 5 guardado en model_GAT_SIR_4.pt\n",
      "Epoch 100: Train Loss: 0.41047993302345276, Val Loss: 0.46289193630218506\n",
      "Epoch 200: Train Loss: 0.4007420241832733, Val Loss: 0.4554305970668793\n",
      "Epoch 300: Train Loss: 0.3964586853981018, Val Loss: 0.4523791968822479\n",
      "Epoch 400: Train Loss: 0.39407432079315186, Val Loss: 0.45092132687568665\n",
      "Epoch 500: Train Loss: 0.3926028311252594, Val Loss: 0.45022642612457275\n",
      "Epoch 600: Train Loss: 0.391629695892334, Val Loss: 0.44990411400794983\n",
      "Epoch 700: Train Loss: 0.39095306396484375, Val Loss: 0.4497537314891815\n",
      "Epoch 800: Train Loss: 0.3904649317264557, Val Loss: 0.4496724307537079\n",
      "Epoch 900: Train Loss: 0.39010247588157654, Val Loss: 0.4496102035045624\n",
      "Epoch 1000: Train Loss: 0.38982707262039185, Val Loss: 0.4495449960231781\n",
      "Best Val Loss: 0.4495449960231781\n",
      "Modelo GAT SIR 6 guardado en model_GAT_SIR_5.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Configura para usar la CPU y obtener mensajes de error más claros\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Suponiendo que tus modelos y datos están listos y que el modelo ya está definido en otro lugar como 'model'\n",
    "modelos_paths_GAT_SIR = []\n",
    "\n",
    "# Asegúrate de que cada Data object en data_list_SIR tenga train_mask, val_mask y test_mask\n",
    "for data in data_list_SIR:\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Asignar aleatoriamente 80% de los nodos para entrenamiento, 10% para validación, 10% para prueba\n",
    "    perm = torch.randperm(num_nodes)\n",
    "    train_mask[perm[:int(0.8 * num_nodes)]] = True\n",
    "    val_mask[perm[int(0.8 * num_nodes):int(0.9 * num_nodes)]] = True\n",
    "    test_mask[perm[int(0.9 * num_nodes):]] = True\n",
    "\n",
    "    # Agregar los masks al objeto data\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def validate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "    return loss.item()\n",
    "\n",
    "# Asumiendo que el modelo y los datos ya están definidos\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for idx, data in enumerate(data_list_SIR):\n",
    "    data = data.to(device)\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        train_loss = train(data)\n",
    "        val_loss = validate(data)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "\n",
    "    # Siempre imprimir la mejor pérdida de validación al final del entrenamiento\n",
    "    print(f'Best Val Loss: {best_val_loss}')\n",
    "    model_path = f'model_GAT_SIR_{idx}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    modelos_paths_GAT_SIR.append(model_path)\n",
    "    print(f'Modelo GAT SIR {idx+1} guardado en {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1cfa099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss: 0.13900607824325562, Val Loss: 0.317048043012619\n",
      "Epoch 200: Train Loss: 0.1257943958044052, Val Loss: 0.3092209994792938\n",
      "Epoch 300: Train Loss: 0.11812405288219452, Val Loss: 0.3076991140842438\n",
      "Epoch 400: Train Loss: 0.11331947147846222, Val Loss: 0.30812570452690125\n",
      "Epoch 500: Train Loss: 0.11013782769441605, Val Loss: 0.30911415815353394\n",
      "Epoch 600: Train Loss: 0.10794460028409958, Val Loss: 0.31023505330085754\n",
      "Epoch 700: Train Loss: 0.1063888743519783, Val Loss: 0.31133440136909485\n",
      "Epoch 800: Train Loss: 0.10526067763566971, Val Loss: 0.31235697865486145\n",
      "Epoch 900: Train Loss: 0.10442707687616348, Val Loss: 0.3132849931716919\n",
      "Epoch 1000: Train Loss: 0.10380098968744278, Val Loss: 0.3141140639781952\n",
      "Best Val Loss: 0.30769097805023193\n",
      "Modelo GAT IC 1 guardado en model_GAT_IC_0.pt\n",
      "Epoch 100: Train Loss: 0.5557520389556885, Val Loss: 0.588675856590271\n",
      "Epoch 200: Train Loss: 0.5395309329032898, Val Loss: 0.5881859064102173\n",
      "Epoch 300: Train Loss: 0.5320850610733032, Val Loss: 0.5890741348266602\n",
      "Epoch 400: Train Loss: 0.527533233165741, Val Loss: 0.5899008512496948\n",
      "Epoch 500: Train Loss: 0.5244181752204895, Val Loss: 0.5905562043190002\n",
      "Epoch 600: Train Loss: 0.5221558809280396, Val Loss: 0.5911163687705994\n",
      "Epoch 700: Train Loss: 0.5204561352729797, Val Loss: 0.5916239619255066\n",
      "Epoch 800: Train Loss: 0.5191519260406494, Val Loss: 0.5920863151550293\n",
      "Epoch 900: Train Loss: 0.5181373357772827, Val Loss: 0.5924966931343079\n",
      "Epoch 1000: Train Loss: 0.5173407793045044, Val Loss: 0.5928493142127991\n",
      "Best Val Loss: 0.5880175232887268\n",
      "Modelo GAT IC 2 guardado en model_GAT_IC_1.pt\n",
      "Epoch 100: Train Loss: 0.6110889911651611, Val Loss: 0.709683358669281\n",
      "Epoch 200: Train Loss: 0.5953606367111206, Val Loss: 0.7157775163650513\n",
      "Epoch 300: Train Loss: 0.588809609413147, Val Loss: 0.7172963619232178\n",
      "Epoch 400: Train Loss: 0.585189700126648, Val Loss: 0.7175294756889343\n",
      "Epoch 500: Train Loss: 0.5829717516899109, Val Loss: 0.7173482775688171\n",
      "Epoch 600: Train Loss: 0.581529438495636, Val Loss: 0.7170353531837463\n",
      "Epoch 700: Train Loss: 0.5805512070655823, Val Loss: 0.7166949510574341\n",
      "Epoch 800: Train Loss: 0.5798656344413757, Val Loss: 0.7163675427436829\n",
      "Epoch 900: Train Loss: 0.5793731212615967, Val Loss: 0.7160698771476746\n",
      "Epoch 1000: Train Loss: 0.5790127515792847, Val Loss: 0.7158074975013733\n",
      "Best Val Loss: 0.6729050278663635\n",
      "Modelo GAT IC 3 guardado en model_GAT_IC_2.pt\n",
      "Epoch 100: Train Loss: 0.11420753598213196, Val Loss: 0.08227115124464035\n",
      "Epoch 200: Train Loss: 0.10635647177696228, Val Loss: 0.07953248918056488\n",
      "Epoch 300: Train Loss: 0.10263852030038834, Val Loss: 0.07796800881624222\n",
      "Epoch 400: Train Loss: 0.10041205585002899, Val Loss: 0.07686924189329147\n",
      "Epoch 500: Train Loss: 0.09896677732467651, Val Loss: 0.07606864720582962\n",
      "Epoch 600: Train Loss: 0.09798092395067215, Val Loss: 0.07546355575323105\n",
      "Epoch 700: Train Loss: 0.0972856730222702, Val Loss: 0.07500173896551132\n",
      "Epoch 800: Train Loss: 0.09678371995687485, Val Loss: 0.07465168088674545\n",
      "Epoch 900: Train Loss: 0.09641528874635696, Val Loss: 0.07438976317644119\n",
      "Epoch 1000: Train Loss: 0.09614190459251404, Val Loss: 0.07419639825820923\n",
      "Best Val Loss: 0.07419639825820923\n",
      "Modelo GAT IC 4 guardado en model_GAT_IC_3.pt\n",
      "Epoch 100: Train Loss: 0.4644645154476166, Val Loss: 0.4774238169193268\n",
      "Epoch 200: Train Loss: 0.4572465121746063, Val Loss: 0.469950407743454\n",
      "Epoch 300: Train Loss: 0.4540828466415405, Val Loss: 0.4671734571456909\n",
      "Epoch 400: Train Loss: 0.4523155093193054, Val Loss: 0.46599242091178894\n",
      "Epoch 500: Train Loss: 0.45120564103126526, Val Loss: 0.46545279026031494\n",
      "Epoch 600: Train Loss: 0.4504610300064087, Val Loss: 0.4652109146118164\n",
      "Epoch 700: Train Loss: 0.44993963837623596, Val Loss: 0.46511971950531006\n",
      "Epoch 800: Train Loss: 0.4495637118816376, Val Loss: 0.46510639786720276\n",
      "Epoch 900: Train Loss: 0.4492868185043335, Val Loss: 0.46513259410858154\n",
      "Epoch 1000: Train Loss: 0.4490799009799957, Val Loss: 0.4651758372783661\n",
      "Best Val Loss: 0.46510493755340576\n",
      "Modelo GAT IC 5 guardado en model_GAT_IC_4.pt\n",
      "Epoch 100: Train Loss: 0.6788268685340881, Val Loss: 0.7038536667823792\n",
      "Epoch 200: Train Loss: 0.6647999882698059, Val Loss: 0.6932154297828674\n",
      "Epoch 300: Train Loss: 0.6583179235458374, Val Loss: 0.6875179409980774\n",
      "Epoch 400: Train Loss: 0.6545881628990173, Val Loss: 0.6838927865028381\n",
      "Epoch 500: Train Loss: 0.6522023677825928, Val Loss: 0.6814802289009094\n",
      "Epoch 600: Train Loss: 0.6505843997001648, Val Loss: 0.6798509955406189\n",
      "Epoch 700: Train Loss: 0.6494432091712952, Val Loss: 0.6787447333335876\n",
      "Epoch 800: Train Loss: 0.6486142873764038, Val Loss: 0.6779935956001282\n",
      "Epoch 900: Train Loss: 0.6479977965354919, Val Loss: 0.6774871945381165\n",
      "Epoch 1000: Train Loss: 0.6475304961204529, Val Loss: 0.6771507859230042\n",
      "Best Val Loss: 0.6771507859230042\n",
      "Modelo GAT IC 6 guardado en model_GAT_IC_5.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Configura para usar la CPU y obtener mensajes de error más claros\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Suponiendo que tus modelos y datos están listos y que el modelo ya está definido en otro lugar como 'model'\n",
    "modelos_paths_GAT_IC = []\n",
    "\n",
    "# Asegúrate de que cada Data object en data_list_IC tenga train_mask, val_mask y test_mask\n",
    "for data in data_list_IC:\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Asignar aleatoriamente 80% de los nodos para entrenamiento, 10% para validación, 10% para prueba\n",
    "    perm = torch.randperm(num_nodes)\n",
    "    train_mask[perm[:int(0.8 * num_nodes)]] = True\n",
    "    val_mask[perm[int(0.8 * num_nodes):int(0.9 * num_nodes)]] = True\n",
    "    test_mask[perm[int(0.9 * num_nodes):]] = True\n",
    "\n",
    "    # Agregar los masks al objeto data\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def validate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "    return loss.item()\n",
    "\n",
    "# Asumiendo que el modelo y los datos ya están definidos\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for idx, data in enumerate(data_list_IC):\n",
    "    data = data.to(device)\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        train_loss = train(data)\n",
    "        val_loss = validate(data)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "\n",
    "    # Siempre imprimir la mejor pérdida de validación al final del entrenamiento\n",
    "    print(f'Best Val Loss: {best_val_loss}')\n",
    "    model_path = f'model_GAT_IC_{idx}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    modelos_paths_GAT_IC.append(model_path)\n",
    "    print(f'Modelo GAT IC {idx+1} guardado en {model_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32828d",
   "metadata": {},
   "source": [
    "def cargar_modelo_SIR(i, in_features, out_features, num_heads, device):\n",
    "    # Asegúrate de que estas dimensiones coincidan con las usadas durante el entrenamiento\n",
    "    model = GAT(in_features=in_features, out_features=out_features, num_heads=num_heads)\n",
    "    model.load_state_dict(torch.load(modelos_paths_SIR[i]))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_IC(i, in_features, out_features, num_heads, device):\n",
    "    model = GAT(in_features=in_features, out_features=out_features, num_heads=num_heads)\n",
    "    model.load_state_dict(torch.load(modelos_paths_IC[i]))  # Asegúrate que el path es correcto\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5520736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_GAT_SIR(i, in_features, out_features, num_heads, device):\n",
    "    model_path = modelos_paths_GAT_SIR[i]\n",
    "    model = GAT(in_features=in_features, out_features=out_features, num_heads=num_heads)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GAT_IC(i, in_features, out_features, num_heads, device):\n",
    "    model_path = modelos_paths_GAT_IC[i]\n",
    "    model = GAT(in_features=in_features, out_features=out_features, num_heads=num_heads)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bff4e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor red de entrenamiento GAT con SIR es la número 4, con un promedio de influencia normalizada de 7.631611210917664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\" Verifica si el objeto es un string que representa un dígito. \"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "# Asume que cargar_modelo_GAT está correctamente definida para cargar tu modelo GAT\n",
    "modelos_entrenados_GAT = [\n",
    "    cargar_modelo_GAT_SIR(i, in_features=128, out_features=32, num_heads=2, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    for i in range(len(modelos_paths_GAT_SIR))\n",
    "]\n",
    "\n",
    "resultados_influencia_GAT = np.zeros((len(modelos_entrenados_GAT), len(redes_test)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_GAT):\n",
    "    for j, data_test in enumerate(redes_test):\n",
    "        if isinstance(data_test, Data):\n",
    "            G_test = to_networkx(data_test, to_undirected=True)\n",
    "        elif isinstance(data_test, nx.Graph):\n",
    "            G_test = data_test\n",
    "        else:\n",
    "            print(f\"Error: Grafo {j} no es compatible.\")\n",
    "            continue\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'embeddings_{j}_test.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test.x, data_test.edge_index)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas)  # Asegúrate de tener una función de simulación adecuada para GAT\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_GAT[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_GAT = resultados_influencia_GAT.mean(axis=1)\n",
    "mejor_red_idx_GAT_SIR = np.argmax(promedio_influencia_por_red_GAT)\n",
    "print(f\"La mejor red de entrenamiento GAT con SIR es la número {mejor_red_idx_GAT_SIR + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_GAT[mejor_red_idx_GAT_SIR]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0080ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor red de entrenamiento GAT con IC es la número 5, con un promedio de influencia normalizada de 0.8477902775127635\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\" Verifica si el objeto es un string que representa un dígito. \"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "# Asume que cargar_modelo_GAT está correctamente definida para cargar tu modelo GAT\n",
    "modelos_entrenados_GAT = [\n",
    "    cargar_modelo_GAT_IC(i, in_features=128, out_features=32, num_heads=2, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    for i in range(len(modelos_paths_GAT_IC))\n",
    "]\n",
    "\n",
    "resultados_influencia_GAT = np.zeros((len(modelos_entrenados_GAT), len(redes_test)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_GAT):\n",
    "    for j, data_test in enumerate(redes_test):\n",
    "        if isinstance(data_test, Data):\n",
    "            G_test = to_networkx(data_test, to_undirected=True)\n",
    "        elif isinstance(data_test, nx.Graph):\n",
    "            G_test = data_test\n",
    "        else:\n",
    "            print(f\"Error: Grafo {j} no es compatible.\")\n",
    "            continue\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'embeddings_{j}_test.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test.x, data_test.edge_index)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_IC(G_test, semillas)  # Asegúrate de tener una función de simulación adecuada para GAT\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_GAT[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_GAT = resultados_influencia_GAT.mean(axis=1)\n",
    "mejor_red_idx_GAT_IC = np.argmax(promedio_influencia_por_red_GAT)\n",
    "print(f\"La mejor red de entrenamiento GAT con IC es la número {mejor_red_idx_GAT_IC + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_GAT[mejor_red_idx_GAT_IC]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09f251",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv, Sequential\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = Sequential('x, edge_index', [\n",
    "            (SAGEConv(in_features, hidden_dim), 'x, edge_index -> x'),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            (SAGEConv(hidden_dim, hidden_dim), 'x, edge_index -> x'),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            (SAGEConv(hidden_dim, out_features), 'x, edge_index -> x')\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.layers(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330a631",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_features)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Añade dropout con una probabilidad del 50%\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e8b4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features, num_layers=3):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        # Capa de entrada\n",
    "        self.convs.append(SAGEConv(in_features, hidden_dim))\n",
    "        \n",
    "        # Capas intermedias\n",
    "        for i in range(1, num_layers-1):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Capa de salida\n",
    "        self.convs.append(SAGEConv(hidden_dim, out_features))\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Añade dropout con una probabilidad del 50%\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b12b91",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Configura para usar la CPU o GPU según esté disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Lista para almacenar las rutas de los modelos entrenados\n",
    "modelos_paths_SIR = []\n",
    "\n",
    "# Asegúrate de que cada objeto Data en `data_list_SIR` tenga `train_mask`, `val_mask` y `test_mask`\n",
    "for data in data_list_SIR:\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Asignar aleatoriamente 80% para entrenamiento, 10% para validación y 10% para prueba\n",
    "    perm = torch.randperm(num_nodes)\n",
    "    train_mask[perm[:int(0.8 * num_nodes)]] = True\n",
    "    val_mask[perm[int(0.8 * num_nodes):int(0.9 * num_nodes)]] = True\n",
    "    test_mask[perm[int(0.9 * num_nodes):]] = True\n",
    "\n",
    "    # Agregar las máscaras al objeto data\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask].to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Función para validar el modelo\n",
    "def validate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.val_mask], data.y[data.val_mask].to(device))\n",
    "    return loss.item()\n",
    "\n",
    "# Instancia de GraphSAGE\n",
    "model = GraphSAGE(in_features=128, hidden_dim=64, out_features=2).to(device)\n",
    "\n",
    "# Optimización y criterio de pérdida\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Proceso de entrenamiento y validación para cada dataset en `data_list_SIR`\n",
    "for idx, data in enumerate(data_list_SIR):\n",
    "    data = data.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Entrenar durante un número determinado de épocas\n",
    "    for epoch in range(1000):\n",
    "        train_loss = train(data)\n",
    "        val_loss = validate(data)\n",
    "\n",
    "        # Guardar el mejor modelo basado en la validación\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        # Reportar cada 100 épocas\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}: Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "    # Imprimir la mejor pérdida de validación al final\n",
    "    print(f'Best Val Loss: {best_val_loss}')\n",
    "    model_path = f'model_GraphSAGE_SIR_{idx}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    modelos_paths_SIR.append(model_path)\n",
    "    print(f'Modelo GraphSAGE SIR {idx + 1} guardado en {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c587b5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss: 0.3578612208366394, Val Loss: 0.4843791723251343\n",
      "Epoch 200: Train Loss: 0.35700908303260803, Val Loss: 0.48934581875801086\n",
      "Epoch 300: Train Loss: 0.35700035095214844, Val Loss: 0.48426494002342224\n",
      "Epoch 400: Train Loss: 0.357101708650589, Val Loss: 0.4887193739414215\n",
      "Epoch 500: Train Loss: 0.35730427503585815, Val Loss: 0.48866409063339233\n",
      "Epoch 600: Train Loss: 0.3568708896636963, Val Loss: 0.4891139268875122\n",
      "Epoch 700: Train Loss: 0.3579598367214203, Val Loss: 0.48832058906555176\n",
      "Epoch 800: Train Loss: 0.3576521575450897, Val Loss: 0.48990967869758606\n",
      "Epoch 900: Train Loss: 0.3568302392959595, Val Loss: 0.4892083406448364\n",
      "Epoch 1000: Train Loss: 0.3593212068080902, Val Loss: 0.4809102714061737\n",
      "Modelo GraphSAGE SIR 1 guardado en model_GraphSAGE_SIR_0.pt\n",
      "Epoch 100: Train Loss: 0.3819368779659271, Val Loss: 0.4818965196609497\n",
      "Epoch 200: Train Loss: 0.38176795840263367, Val Loss: 0.4810703992843628\n",
      "Epoch 300: Train Loss: 0.3807585835456848, Val Loss: 0.4827616214752197\n",
      "Epoch 400: Train Loss: 0.38210251927375793, Val Loss: 0.4842192530632019\n",
      "Epoch 500: Train Loss: 0.383280485868454, Val Loss: 0.4773908257484436\n",
      "Epoch 600: Train Loss: 0.3810209035873413, Val Loss: 0.47963234782218933\n",
      "Epoch 700: Train Loss: 0.3820021152496338, Val Loss: 0.48043566942214966\n",
      "Epoch 800: Train Loss: 0.38281702995300293, Val Loss: 0.4792199730873108\n",
      "Epoch 900: Train Loss: 0.38128072023391724, Val Loss: 0.4817677438259125\n",
      "Epoch 1000: Train Loss: 0.3823303282260895, Val Loss: 0.4806913733482361\n",
      "Modelo GraphSAGE SIR 2 guardado en model_GraphSAGE_SIR_1.pt\n",
      "Epoch 100: Train Loss: 0.4257013201713562, Val Loss: 0.43967434763908386\n",
      "Epoch 200: Train Loss: 0.4253368377685547, Val Loss: 0.4396754205226898\n",
      "Epoch 300: Train Loss: 0.42515724897384644, Val Loss: 0.43980664014816284\n",
      "Epoch 400: Train Loss: 0.42459210753440857, Val Loss: 0.4398498833179474\n",
      "Epoch 500: Train Loss: 0.4248793125152588, Val Loss: 0.43985527753829956\n",
      "Epoch 600: Train Loss: 0.42579567432403564, Val Loss: 0.43969017267227173\n",
      "Epoch 700: Train Loss: 0.42475688457489014, Val Loss: 0.4398556649684906\n",
      "Epoch 800: Train Loss: 0.4256284832954407, Val Loss: 0.4396669864654541\n",
      "Epoch 900: Train Loss: 0.42523249983787537, Val Loss: 0.43969810009002686\n",
      "Epoch 1000: Train Loss: 0.4248921871185303, Val Loss: 0.43965283036231995\n",
      "Modelo GraphSAGE SIR 3 guardado en model_GraphSAGE_SIR_2.pt\n",
      "Epoch 100: Train Loss: 0.4755484461784363, Val Loss: 0.44689294695854187\n",
      "Epoch 200: Train Loss: 0.4759537875652313, Val Loss: 0.4473428726196289\n",
      "Epoch 300: Train Loss: 0.4761234521865845, Val Loss: 0.4469236731529236\n",
      "Epoch 400: Train Loss: 0.4758323132991791, Val Loss: 0.4463525414466858\n",
      "Epoch 500: Train Loss: 0.47577524185180664, Val Loss: 0.4466007351875305\n",
      "Epoch 600: Train Loss: 0.4758906960487366, Val Loss: 0.44598811864852905\n",
      "Epoch 700: Train Loss: 0.47668033838272095, Val Loss: 0.44601690769195557\n",
      "Epoch 800: Train Loss: 0.4757346212863922, Val Loss: 0.4465691149234772\n",
      "Epoch 900: Train Loss: 0.47600409388542175, Val Loss: 0.44630101323127747\n",
      "Epoch 1000: Train Loss: 0.47545111179351807, Val Loss: 0.4465706944465637\n",
      "Modelo GraphSAGE SIR 4 guardado en model_GraphSAGE_SIR_3.pt\n",
      "Epoch 100: Train Loss: 0.4286114573478699, Val Loss: 0.4120189845561981\n",
      "Epoch 200: Train Loss: 0.4288296103477478, Val Loss: 0.41155165433883667\n",
      "Epoch 300: Train Loss: 0.42882898449897766, Val Loss: 0.41169628500938416\n",
      "Epoch 400: Train Loss: 0.4285743832588196, Val Loss: 0.4113856256008148\n",
      "Epoch 500: Train Loss: 0.4293213486671448, Val Loss: 0.41188567876815796\n",
      "Epoch 600: Train Loss: 0.4287429749965668, Val Loss: 0.4112759530544281\n",
      "Epoch 700: Train Loss: 0.42888835072517395, Val Loss: 0.41180485486984253\n",
      "Epoch 800: Train Loss: 0.43062928318977356, Val Loss: 0.41225066781044006\n",
      "Epoch 900: Train Loss: 0.42925629019737244, Val Loss: 0.4123603105545044\n",
      "Epoch 1000: Train Loss: 0.42854365706443787, Val Loss: 0.41377168893814087\n",
      "Modelo GraphSAGE SIR 5 guardado en model_GraphSAGE_SIR_4.pt\n",
      "Epoch 100: Train Loss: 0.39908280968666077, Val Loss: 0.44107773900032043\n",
      "Epoch 200: Train Loss: 0.3983657956123352, Val Loss: 0.4413394629955292\n",
      "Epoch 300: Train Loss: 0.39836570620536804, Val Loss: 0.44291573762893677\n",
      "Epoch 400: Train Loss: 0.3987100422382355, Val Loss: 0.44237402081489563\n",
      "Epoch 500: Train Loss: 0.3985487222671509, Val Loss: 0.4428829550743103\n",
      "Epoch 600: Train Loss: 0.39807501435279846, Val Loss: 0.4406510889530182\n",
      "Epoch 700: Train Loss: 0.39817994832992554, Val Loss: 0.4416603744029999\n",
      "Epoch 800: Train Loss: 0.3975907862186432, Val Loss: 0.44240182638168335\n",
      "Epoch 900: Train Loss: 0.3988877236843109, Val Loss: 0.44104257225990295\n",
      "Epoch 1000: Train Loss: 0.39811593294143677, Val Loss: 0.441024512052536\n",
      "Modelo GraphSAGE SIR 6 guardado en model_GraphSAGE_SIR_5.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Configura para usar la CPU o GPU según esté disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Lista para almacenar las rutas de los modelos entrenados\n",
    "modelos_paths_GraphSAGE_SIR = []\n",
    "\n",
    "# Asegúrate de que cada objeto Data en `data_list_SIR` tenga `train_mask`, `val_mask` y `test_mask`\n",
    "for data in data_list_SIR:\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Asignar aleatoriamente 80% para entrenamiento, 10% para validación y 10% para prueba\n",
    "    perm = torch.randperm(num_nodes)\n",
    "    train_mask[perm[:int(0.8 * num_nodes)]] = True\n",
    "    val_mask[perm[int(0.8 * num_nodes):int(0.9 * num_nodes)]] = True\n",
    "    test_mask[perm[int(0.9 * num_nodes):]] = True\n",
    "\n",
    "    # Agregar las máscaras al objeto data\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask].to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Función para validar el modelo\n",
    "def validate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.val_mask], data.y[data.val_mask].to(device))\n",
    "    return loss.item()\n",
    "\n",
    "# Instancia de GraphSAGE\n",
    "model = GraphSAGE(in_features=128, hidden_dim=64, out_features=2, num_layers=5).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-3)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#patience = 150\n",
    "#patience_counter = 0\n",
    "\n",
    "# Proceso de entrenamiento y validación para cada dataset en `data_list_SIR`\n",
    "for idx, data in enumerate(data_list_SIR):\n",
    "    data = data.to(device)\n",
    "    #best_val_loss = float('inf')\n",
    "\n",
    "    # Entrenar durante un número determinado de épocas\n",
    "    for epoch in range(1000):\n",
    "        train_loss = train(data)\n",
    "        val_loss = validate(data)\n",
    "        #scheduler.step(val_loss)\n",
    "\n",
    "        #if val_loss < best_val_loss:\n",
    "         #   best_val_loss = val_loss\n",
    "          #  patience_counter = 0  # reset counter if improvement is seen\n",
    "        #else:\n",
    "         #   patience_counter += 1\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {train_loss}, Val Loss: {val_loss}')    \n",
    "        \n",
    "        #if patience_counter >= patience:\n",
    "         #   print(\"Early stopping triggered.\")\n",
    "          #  break\n",
    "\n",
    "\n",
    "    #print(f'Best Val Loss: {best_val_loss}')\n",
    "    model_path = f'model_GraphSAGE_SIR_{idx}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    modelos_paths_GraphSAGE_SIR.append(model_path)\n",
    "    print(f'Modelo GraphSAGE SIR {idx+1} guardado en {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "528a1ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss: 0.15050072968006134, Val Loss: 0.2367667257785797\n",
      "Epoch 200: Train Loss: 0.15076451003551483, Val Loss: 0.23334266245365143\n",
      "Epoch 300: Train Loss: 0.15054138004779816, Val Loss: 0.2335858792066574\n",
      "Epoch 400: Train Loss: 0.1501556634902954, Val Loss: 0.23445402085781097\n",
      "Epoch 500: Train Loss: 0.1484910249710083, Val Loss: 0.2349425107240677\n",
      "Epoch 600: Train Loss: 0.14854800701141357, Val Loss: 0.23375597596168518\n",
      "Epoch 700: Train Loss: 0.14917144179344177, Val Loss: 0.2331269383430481\n",
      "Epoch 800: Train Loss: 0.14923995733261108, Val Loss: 0.23254740238189697\n",
      "Epoch 900: Train Loss: 0.14931020140647888, Val Loss: 0.23521775007247925\n",
      "Epoch 1000: Train Loss: 0.14880457520484924, Val Loss: 0.23389650881290436\n",
      "Modelo GraphSAGE IC 1 guardado en model_GraphSAGE_IC_0.pt\n",
      "Epoch 100: Train Loss: 0.5720323920249939, Val Loss: 0.5308099985122681\n",
      "Epoch 200: Train Loss: 0.5716090202331543, Val Loss: 0.5311988592147827\n",
      "Epoch 300: Train Loss: 0.5700216889381409, Val Loss: 0.5295248627662659\n",
      "Epoch 400: Train Loss: 0.5717874765396118, Val Loss: 0.5317519903182983\n",
      "Epoch 500: Train Loss: 0.5717427730560303, Val Loss: 0.5316505432128906\n",
      "Epoch 600: Train Loss: 0.5719327330589294, Val Loss: 0.5324103236198425\n",
      "Epoch 700: Train Loss: 0.5717583298683167, Val Loss: 0.5315172076225281\n",
      "Epoch 800: Train Loss: 0.5717876553535461, Val Loss: 0.5315879583358765\n",
      "Epoch 900: Train Loss: 0.5717649459838867, Val Loss: 0.5315163135528564\n",
      "Epoch 1000: Train Loss: 0.5717718005180359, Val Loss: 0.5315201878547668\n",
      "Modelo GraphSAGE IC 2 guardado en model_GraphSAGE_IC_1.pt\n",
      "Epoch 100: Train Loss: 0.652336835861206, Val Loss: 0.6303154230117798\n",
      "Epoch 200: Train Loss: 0.6521450877189636, Val Loss: 0.630363941192627\n",
      "Epoch 300: Train Loss: 0.6519683599472046, Val Loss: 0.6302534341812134\n",
      "Epoch 400: Train Loss: 0.6520676612854004, Val Loss: 0.6307988166809082\n",
      "Epoch 500: Train Loss: 0.6519711017608643, Val Loss: 0.6302137970924377\n",
      "Epoch 600: Train Loss: 0.6519706845283508, Val Loss: 0.6302237510681152\n",
      "Epoch 700: Train Loss: 0.6519716382026672, Val Loss: 0.6302236318588257\n",
      "Epoch 800: Train Loss: 0.6519652605056763, Val Loss: 0.6302834749221802\n",
      "Epoch 900: Train Loss: 0.6518955826759338, Val Loss: 0.6299286484718323\n",
      "Epoch 1000: Train Loss: 0.6519870758056641, Val Loss: 0.6303348541259766\n",
      "Modelo GraphSAGE IC 3 guardado en model_GraphSAGE_IC_2.pt\n",
      "Epoch 100: Train Loss: 0.14534978568553925, Val Loss: 0.07650048285722733\n",
      "Epoch 200: Train Loss: 0.11089932918548584, Val Loss: 0.07759881019592285\n",
      "Epoch 300: Train Loss: 0.10961278527975082, Val Loss: 0.07813861221075058\n",
      "Epoch 400: Train Loss: 0.10729237645864487, Val Loss: 0.07702946662902832\n",
      "Epoch 500: Train Loss: 0.10461503267288208, Val Loss: 0.07353033125400543\n",
      "Epoch 600: Train Loss: 0.1061314046382904, Val Loss: 0.07245499640703201\n",
      "Epoch 700: Train Loss: 0.1062416210770607, Val Loss: 0.0721820667386055\n",
      "Epoch 800: Train Loss: 0.1482296884059906, Val Loss: 0.0897708311676979\n",
      "Epoch 900: Train Loss: 0.10731088370084763, Val Loss: 0.07346907258033752\n",
      "Epoch 1000: Train Loss: 0.10481571406126022, Val Loss: 0.07341589033603668\n",
      "Modelo GraphSAGE IC 4 guardado en model_GraphSAGE_IC_3.pt\n",
      "Epoch 100: Train Loss: 0.48307937383651733, Val Loss: 0.5152420997619629\n",
      "Epoch 200: Train Loss: 0.48227304220199585, Val Loss: 0.5152958631515503\n",
      "Epoch 300: Train Loss: 0.48222631216049194, Val Loss: 0.515486478805542\n",
      "Epoch 400: Train Loss: 0.4824913442134857, Val Loss: 0.514972448348999\n",
      "Epoch 500: Train Loss: 0.4827048182487488, Val Loss: 0.5154377222061157\n",
      "Epoch 600: Train Loss: 0.4825918674468994, Val Loss: 0.5150396823883057\n",
      "Epoch 700: Train Loss: 0.4825468957424164, Val Loss: 0.5149928331375122\n",
      "Epoch 800: Train Loss: 0.48329198360443115, Val Loss: 0.5152804851531982\n",
      "Epoch 900: Train Loss: 0.48308736085891724, Val Loss: 0.5150533318519592\n",
      "Epoch 1000: Train Loss: 0.48256999254226685, Val Loss: 0.5154030919075012\n",
      "Modelo GraphSAGE IC 5 guardado en model_GraphSAGE_IC_4.pt\n",
      "Epoch 100: Train Loss: 0.6692126393318176, Val Loss: 0.6687888503074646\n",
      "Epoch 200: Train Loss: 0.6691299676895142, Val Loss: 0.668751060962677\n",
      "Epoch 300: Train Loss: 0.6691220998764038, Val Loss: 0.6687561273574829\n",
      "Epoch 400: Train Loss: 0.669093906879425, Val Loss: 0.6687701344490051\n",
      "Epoch 500: Train Loss: 0.6691218018531799, Val Loss: 0.6687561273574829\n",
      "Epoch 600: Train Loss: 0.6691214442253113, Val Loss: 0.6687564253807068\n",
      "Epoch 700: Train Loss: 0.6691229939460754, Val Loss: 0.6687570214271545\n",
      "Epoch 800: Train Loss: 0.6690986156463623, Val Loss: 0.6687544584274292\n",
      "Epoch 900: Train Loss: 0.668958306312561, Val Loss: 0.6687684655189514\n",
      "Epoch 1000: Train Loss: 0.6691214442253113, Val Loss: 0.6687555909156799\n",
      "Modelo GraphSAGE IC 6 guardado en model_GraphSAGE_IC_5.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Configura para usar la CPU o GPU según esté disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Lista para almacenar las rutas de los modelos entrenados\n",
    "modelos_paths_GraphSAGE_IC = []\n",
    "\n",
    "# Asegúrate de que cada objeto Data en `data_list_IC` tenga `train_mask`, `val_mask` y `test_mask`\n",
    "for data in data_list_IC:\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Asignar aleatoriamente 80% para entrenamiento, 10% para validación y 10% para prueba\n",
    "    perm = torch.randperm(num_nodes)\n",
    "    train_mask[perm[:int(0.8 * num_nodes)]] = True\n",
    "    val_mask[perm[int(0.8 * num_nodes):int(0.9 * num_nodes)]] = True\n",
    "    test_mask[perm[int(0.9 * num_nodes):]] = True\n",
    "\n",
    "    # Agregar las máscaras al objeto data\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask].to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Función para validar el modelo\n",
    "def validate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.val_mask], data.y[data.val_mask].to(device))\n",
    "    return loss.item()\n",
    "\n",
    "# Instancia de GraphSAGE\n",
    "model = GraphSAGE(in_features=128, hidden_dim=64, out_features=2, num_layers=5).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-3)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#patience = 150\n",
    "#patience_counter = 0\n",
    "\n",
    "# Proceso de entrenamiento y validación para cada dataset en `data_list_IC`\n",
    "for idx, data in enumerate(data_list_IC):\n",
    "    data = data.to(device)\n",
    "    #best_val_loss = float('inf')\n",
    "\n",
    "    # Entrenar durante un número determinado de épocas\n",
    "    for epoch in range(1000):\n",
    "        train_loss = train(data)\n",
    "        val_loss = validate(data)\n",
    "        #scheduler.step(val_loss)\n",
    "\n",
    "        #if val_loss < best_val_loss:\n",
    "         #   best_val_loss = val_loss\n",
    "          #  patience_counter = 0  # reset counter if improvement is seen\n",
    "        #else:\n",
    "         #   patience_counter += 1\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {train_loss}, Val Loss: {val_loss}')    \n",
    "        \n",
    "        #if patience_counter >= patience:\n",
    "         #   print(\"Early stopping triggered.\")\n",
    "          #  break\n",
    "\n",
    "\n",
    "    #print(f'Best Val Loss: {best_val_loss}')\n",
    "    model_path = f'model_GraphSAGE_IC_{idx}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    modelos_paths_GraphSAGE_IC.append(model_path)\n",
    "    print(f'Modelo GraphSAGE IC {idx+1} guardado en {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fe56cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_GraphSAGE_SIR(i, in_features, hidden_dim, out_features, num_layers, device):\n",
    "    model = GraphSAGE(in_features, hidden_dim, out_features, num_layers)\n",
    "    model_path = modelos_paths_GraphSAGE_SIR[i]\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def cargar_modelo_GraphSAGE_IC(i, in_features, hidden_dim, out_features, num_layers, device):\n",
    "    model = GraphSAGE(in_features, hidden_dim, out_features, num_layers)\n",
    "    model_path = modelos_paths_GraphSAGE_IC[i]\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dadf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor red de entrenamiento GraphSAGE es la número 4, con un promedio de influencia normalizada de 7.427768984609267\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\" Verifica si el objeto es un string que representa un dígito. \"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "# Carga los modelos entrenados\n",
    "modelos_entrenados_SIR = [\n",
    "    cargar_modelo_GraphSAGE_SIR(i, in_features=128, hidden_dim=64, out_features=2, num_layers=5, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    for i in range(len(modelos_paths_GraphSAGE_SIR))\n",
    "]\n",
    "\n",
    "resultados_influencia_GraphSAGE_SIR = np.zeros((len(modelos_entrenados_SIR), len(redes_test)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_SIR):\n",
    "    for j, data_test in enumerate(redes_test):\n",
    "        if isinstance(data_test, Data):\n",
    "            G_test = to_networkx(data_test, to_undirected=True)\n",
    "        elif isinstance(data_test, nx.Graph):\n",
    "            G_test = data_test\n",
    "        else:\n",
    "            print(f\"Error: Grafo {j} no es compatible.\")\n",
    "            continue\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'embeddings_{j}_test.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test.x, data_test.edge_index)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_SIR(G_test, semillas)  # Asegúrate de tener una función de simulación adecuada\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_GraphSAGE_SIR[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_GraphSAGE_SIR = resultados_influencia_GraphSAGE_SIR.mean(axis=1)\n",
    "mejor_red_idx_GraphSAGE_SIR = np.argmax(promedio_influencia_por_red_GraphSAGE_SIR)\n",
    "print(f\"La mejor red de entrenamiento GraphSAGE es la número {mejor_red_idx_GraphSAGE_SIR + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_GraphSAGE_SIR[mejor_red_idx_GraphSAGE_SIR]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f6e3687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor red de entrenamiento GraphSAGE es la número 6, con un promedio de influencia normalizada de 0.8950303775940672\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def is_digit_string(s):\n",
    "    \"\"\" Verifica si el objeto es un string que representa un dígito. \"\"\"\n",
    "    return isinstance(s, str) and s.isdigit()\n",
    "\n",
    "# Carga los modelos entrenados\n",
    "modelos_entrenados_IC = [\n",
    "    cargar_modelo_GraphSAGE_IC(i, in_features=128, hidden_dim=64, out_features=2, num_layers=5, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    for i in range(len(modelos_paths_GraphSAGE_IC))\n",
    "]\n",
    "\n",
    "resultados_influencia_GraphSAGE_IC = np.zeros((len(modelos_entrenados_IC), len(redes_test)))\n",
    "\n",
    "for i, model in enumerate(modelos_entrenados_IC):\n",
    "    for j, data_test in enumerate(redes_test):\n",
    "        if isinstance(data_test, Data):\n",
    "            G_test = to_networkx(data_test, to_undirected=True)\n",
    "        elif isinstance(data_test, nx.Graph):\n",
    "            G_test = data_test\n",
    "        else:\n",
    "            print(f\"Error: Grafo {j} no es compatible.\")\n",
    "            continue\n",
    "\n",
    "        if not G_test.nodes or not G_test.edges:\n",
    "            print(f\"No hay nodos o aristas válidos para el Modelo {i}, Grafo {j}\")\n",
    "            continue\n",
    "\n",
    "        # Convertir nodos a enteros si son strings numéricos\n",
    "        mapping = {node: int(node) if is_digit_string(node) else node for node in G_test.nodes()}\n",
    "        G_test = nx.relabel_nodes(G_test, mapping)\n",
    "\n",
    "        embeddings_test = np.loadtxt(f'embeddings_{j}_test.emb', skiprows=1)\n",
    "        x_test = torch.tensor(embeddings_test[:, 1:], dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(G_test.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data_test = Data(x=x_test, edge_index=edge_index)\n",
    "        data_test = data_test.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_test.x, data_test.edge_index)\n",
    "            influencia_scores = out.softmax(dim=1)[:, 1]\n",
    "\n",
    "        total_infectados_red = 0\n",
    "        for size in range(10, 51, 5):\n",
    "            _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "            semillas = [idx.item() for idx in indices_semillas if idx.item() in G_test.nodes()]\n",
    "\n",
    "            if not semillas:\n",
    "                print(f\"No hay semillas válidas para el Modelo {i}, Grafo {j}, Tamaño {size}\")\n",
    "                continue\n",
    "\n",
    "            infectados_un_tamano = simular_difusion_IC(G_test, semillas)  # Asegúrate de tener una función de simulación adecuada\n",
    "            total_infectados_red += infectados_un_tamano\n",
    "\n",
    "        resultados_influencia_GraphSAGE_IC[i, j] = total_infectados_red / G_test.number_of_nodes()\n",
    "\n",
    "promedio_influencia_por_red_GraphSAGE_IC = resultados_influencia_GraphSAGE_IC.mean(axis=1)\n",
    "mejor_red_idx_GraphSAGE_IC = np.argmax(promedio_influencia_por_red_GraphSAGE_IC)\n",
    "print(f\"La mejor red de entrenamiento GraphSAGE es la número {mejor_red_idx_GraphSAGE_IC + 1}, con un promedio de influencia normalizada de {promedio_influencia_por_red_GraphSAGE_IC[mejor_red_idx_GraphSAGE_IC]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f980c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def seleccionar_semillas(modelo, data, size, model_type):\n",
    "    modelo.eval()\n",
    "    with torch.no_grad():\n",
    "        if model_type == 'GCN':\n",
    "            out = modelo(data)  # Para GNN que espera un objeto Data\n",
    "        else:\n",
    "            out = modelo(data.x, data.edge_index)  # Para GAT y GraphSAGE que esperan x y edge_index por separado\n",
    "        influencia_scores = torch.softmax(out, dim=1)[:, 1]\n",
    "        _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "        semillas = indices_semillas.tolist()\n",
    "    return semillas\n",
    "\n",
    "modelos = {\n",
    "    'GCN': (cargar_modelo_GCN_SIR(mejor_red_idx_GCN_SIR, input_dim=128, hidden_dim=64, output_dim=2, device=device), 'GCN'),\n",
    "    'GAT': (cargar_modelo_GAT_SIR(mejor_red_idx_GAT_SIR, in_features=128, out_features=32, num_heads=2, device=device), 'GAT'),\n",
    "    'GraphSAGE': (cargar_modelo_GraphSAGE_SIR(mejor_red_idx_GraphSAGE_SIR, in_features=128, hidden_dim=64, out_features=2, num_layers=5, device=device), 'GraphSAGE')\n",
    "}\n",
    "\n",
    "sizes = range(10, 51, 5)\n",
    "\n",
    "# Proceso para cada red en el rango de índices de 6 a 13\n",
    "for index in range(6, 14):\n",
    "    resultados = {modelo: [] for modelo in modelos}\n",
    "    G_test = redes_test[index]\n",
    "    G_test = to_networkx(G_test, to_undirected=True)\n",
    "    G_test = nx.relabel_nodes(G_test, {n: int(n) for n in G_test.nodes()})\n",
    "    \n",
    "    for modelo_name, (modelo, model_type) in modelos.items():\n",
    "        for size in sizes:\n",
    "            x = torch.tensor(np.random.rand(len(G_test.nodes()), 128), dtype=torch.float).to(device)\n",
    "            edge_index = torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous().to(device)\n",
    "            data_test = Data(x=x, edge_index=edge_index)\n",
    "            \n",
    "            semillas = seleccionar_semillas(modelo, data_test, size, model_type)\n",
    "            infectados_total = 0\n",
    "            num_runs = 10  # Número de simulaciones por tamaño de semilla para promediar\n",
    "            for _ in range(num_runs):\n",
    "                infectados = simular_difusion_SIR(G_test, semillas)\n",
    "                infectados_total += infectados\n",
    "            influencia_promedio = infectados_total / num_runs / len(G_test.nodes())\n",
    "            resultados[modelo_name].append(influencia_promedio)\n",
    "    \n",
    "    # Graficación de los resultados para cada red\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for modelo_name, valores in resultados.items():\n",
    "        plt.plot(list(sizes), valores, label=modelo_name, marker='o')\n",
    "    plt.title(f'Influencia Promedio en SIR vs. Tamaño de Semillas Iniciales para Red {index}')\n",
    "    plt.xlabel('Número de Semillas Iniciales')\n",
    "    plt.ylabel('Influencia Promedio Normalizada')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./resultados/Grafica_Influencia_SIR_vs_inicial_Red_{index}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c41171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def seleccionar_semillas(modelo, data, size, model_type):\n",
    "    modelo.eval()\n",
    "    with torch.no_grad():\n",
    "        if model_type == 'GCN':\n",
    "            out = modelo(data)  # Para GNN que espera un objeto Data\n",
    "        else:\n",
    "            out = modelo(data.x, data.edge_index)  # Para GAT y GraphSAGE que esperan x y edge_index por separado\n",
    "        influencia_scores = torch.softmax(out, dim=1)[:, 1]\n",
    "        _, indices_semillas = torch.topk(influencia_scores, size)\n",
    "        semillas = indices_semillas.tolist()\n",
    "    return semillas\n",
    "\n",
    "modelos = {\n",
    "    'GCN': (cargar_modelo_GCN_IC(mejor_red_idx_GCN_IC, input_dim=128, hidden_dim=64, output_dim=2, device=device), 'GCN'),\n",
    "    'GAT': (cargar_modelo_GAT_IC(mejor_red_idx_GAT_IC, in_features=128, out_features=32, num_heads=2, device=device), 'GAT'),\n",
    "    'GraphSAGE': (cargar_modelo_GraphSAGE_IC(mejor_red_idx_GraphSAGE_IC, in_features=128, hidden_dim=64, out_features=2, num_layers=5, device=device), 'GraphSAGE')\n",
    "}\n",
    "\n",
    "sizes = range(10, 51, 5)\n",
    "\n",
    "# Proceso para cada red en el rango de índices de 6 a 13\n",
    "for index in range(6, 14):\n",
    "    resultados = {modelo: [] for modelo in modelos}\n",
    "    G_test = redes_test[index]\n",
    "    G_test = to_networkx(G_test, to_undirected=True)\n",
    "    G_test = nx.relabel_nodes(G_test, {n: int(n) for n in G_test.nodes()})\n",
    "    \n",
    "    for modelo_name, (modelo, model_type) in modelos.items():\n",
    "        for size in sizes:\n",
    "            x = torch.tensor(np.random.rand(len(G_test.nodes()), 128), dtype=torch.float).to(device)\n",
    "            edge_index = torch.tensor([[u, v] for u, v in G_test.edges()], dtype=torch.long).t().contiguous().to(device)\n",
    "            data_test = Data(x=x, edge_index=edge_index)\n",
    "            \n",
    "            semillas = seleccionar_semillas(modelo, data_test, size, model_type)\n",
    "            infectados_total = 0\n",
    "            num_runs = 10  # Número de simulaciones por tamaño de semilla para promediar\n",
    "            for _ in range(num_runs):\n",
    "                infectados = simular_difusion_IC(G_test, semillas)\n",
    "                infectados_total += infectados\n",
    "            influencia_promedio = infectados_total / num_runs / len(G_test.nodes())\n",
    "            resultados[modelo_name].append(influencia_promedio)\n",
    "    \n",
    "    # Graficación de los resultados para cada red\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for modelo_name, valores in resultados.items():\n",
    "        plt.plot(list(sizes), valores, label=modelo_name, marker='o')\n",
    "    plt.title(f'Influencia Promedio en IC vs. Tamaño de Semillas Iniciales para Red {index}')\n",
    "    plt.xlabel('Número de Semillas Iniciales')\n",
    "    plt.ylabel('Influencia Promedio Normalizada')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./resultados/Grafica_Influencia_IC_vs_inicial_Red_{index}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2dd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyg_env)",
   "language": "python",
   "name": "pyg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
